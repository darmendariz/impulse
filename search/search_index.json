{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Impulse","text":"<p>A Python library for collecting, parsing, and processing raw Rocket League <code>.replay</code> files into structured data for general analysis or machine learning tasks.</p>"},{"location":"#overview","title":"Overview","text":"<p>Impulse is a flexible API for performing machine learning on Rocket League game data. It provides a complete and customizable pipeline for downloading Rocket League <code>.replay</code> files from Ballchasing.com and extracting ML-ready data from them at frame-level precision. It handles the infrastructure concerns so you can easily create clean datasets from Ballchasing's repository of 140+ million replays in a few lines of code.</p> <p>The library is designed to scale. With it, you can download and process tens of thousands of replays with AWS storage/compute service integration, database registration, automatic progress tracking, failure recovery, and configurable data quality validation.</p>"},{"location":"#feature-overview","title":"Feature Overview","text":"<p>Collection</p> <ul> <li>Ballchasing.com API client with automatic rate limiting and recursive group tree parsing</li> <li>Multiple storage backends: local filesystem or AWS S3</li> <li>SQLite tracking for deduplication and download registration</li> <li>Resume interrupted downloads and retry failures</li> </ul> <p>Parsing</p> <ul> <li>High-performance parsing via subtr-actor (Rust-based)</li> <li>Configurable feature extraction: ball physics, player positions/velocities, boost, and more</li> <li>Multiple output formats: NumPy arrays, pandas DataFrames, Parquet files</li> <li>Built-in data quality validation and feature deduplication</li> </ul> <p>Configuration</p> <ul> <li><code>CollectionConfig</code>: API keys, storage settings, rate limits</li> <li><code>ParsingConfig</code>: Feature extraction presets, custom feature selection, FPS sampling</li> <li><code>PipelineConfig</code>: Data quality thresholds, deduplication strategy, schema settings</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from impulse.collection import download_group\nfrom impulse.parsing import ReplayParser\n\n# Download replays from a Ballchasing group\nresult = download_group(\n    group_id='your-group-id',\n    storage_type='local',\n    output_dir='./replays'\n)\n\n# Parse a replay into a NumPy array\nparser = ReplayParser.from_preset('standard', fps=10.0)\nresult = parser.parse_file('./replays/my_replay.replay')\n\nif result.success:\n    print(f\"Shape: {result.array.shape}\")  # (frames, features)\n</code></pre>"},{"location":"#related-libraries","title":"Related Libraries","text":"Library Language Purpose boxcars Rust Low-level replay parsing \u2014 decodes the binary <code>.replay</code> format subtr-actor Rust Extracts frame-by-frame actor data into NumPy arrays (built on boxcars)"},{"location":"collection/api-reference/","title":"Collection API Reference","text":""},{"location":"collection/api-reference/#ballchasingclient","title":"BallchasingClient","text":""},{"location":"collection/api-reference/#impulse.collection.ballchasing_client.BallchasingClient","title":"BallchasingClient","text":"<pre><code>BallchasingClient(\n    config: CollectionConfig = None,\n    rate_limiter: RateLimiter = None,\n)\n</code></pre> <p>Pure API client for Ballchasing.com.</p> <p>Handles HTTP requests to the Ballchasing API without any orchestration, storage, or database logic. Use this class when you need low-level API access.</p> <p>For high-level download workflows, use ReplayDownloader instead.</p> <p>Initialize Ballchasing API client.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>CollectionConfig</code> <p>Configuration object (defaults to loading from environment)</p> <code>None</code> <code>rate_limiter</code> <code>RateLimiter</code> <p>Optional rate limiter (creates default if not provided)</p> <code>None</code> Source code in <code>impulse/collection/ballchasing_client.py</code> <pre><code>def __init__(self, config: CollectionConfig = None, rate_limiter: RateLimiter = None):\n    \"\"\"\n    Initialize Ballchasing API client.\n\n    Args:\n        config: Configuration object (defaults to loading from environment)\n        rate_limiter: Optional rate limiter (creates default if not provided)\n    \"\"\"\n    if config is None:\n        config = CollectionConfig.from_env()\n\n    self.config = config\n    self.api_key = config.ballchasing_api_key\n    self.base_url = \"https://ballchasing.com/api\"\n    self.rate_limiter = rate_limiter or RateLimiter(\n        requests_per_second=config.rate_limit_per_second,\n        requests_per_hour=config.rate_limit_per_hour\n    )\n\n    # Create HTTP session with auth headers\n    self.session = requests.Session()\n    self.session.headers.update({\"Authorization\": self.api_key})\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.ballchasing_client.BallchasingClient.get_group_info","title":"get_group_info","text":"<pre><code>get_group_info(group_id: str) -&gt; Dict\n</code></pre> <p>Fetch metadata for a Ballchasing group.</p> <p>Returns basic info such as group id, link, name, date created, creator, and player stats from replays in the group.</p> <p>Note: Response does not include information about child groups.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Ballchasing group ID</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dict with group metadata</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails</p> API Documentation <p>https://ballchasing.com/doc/api#replay-groups-group-get</p> Source code in <code>impulse/collection/ballchasing_client.py</code> <pre><code>def get_group_info(self, group_id: str) -&gt; Dict:\n    \"\"\"\n    Fetch metadata for a Ballchasing group.\n\n    Returns basic info such as group id, link, name, date created, creator,\n    and player stats from replays in the group.\n\n    Note: Response does not include information about child groups.\n\n    Args:\n        group_id: Ballchasing group ID\n\n    Returns:\n        Dict with group metadata\n\n    Raises:\n        requests.HTTPError: If the API request fails\n\n    API Documentation:\n        https://ballchasing.com/doc/api#replay-groups-group-get\n    \"\"\"\n    self.rate_limiter.wait_if_needed()\n    url = f\"{self.base_url}/groups/{group_id}\"\n    response = self.session.get(url)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.ballchasing_client.BallchasingClient.get_child_groups","title":"get_child_groups","text":"<pre><code>get_child_groups(parent_group_id: str) -&gt; List[Dict]\n</code></pre> <p>Get all child groups of a parent group.</p> <p>Handles pagination automatically to fetch all child groups.</p> <p>Parameters:</p> Name Type Description Default <code>parent_group_id</code> <code>str</code> <p>Parent group ID</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of child group dicts</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails</p> API Documentation <p>https://ballchasing.com/doc/api#replay-groups-groups-get</p> Source code in <code>impulse/collection/ballchasing_client.py</code> <pre><code>def get_child_groups(self, parent_group_id: str) -&gt; List[Dict]:\n    \"\"\"\n    Get all child groups of a parent group.\n\n    Handles pagination automatically to fetch all child groups.\n\n    Args:\n        parent_group_id: Parent group ID\n\n    Returns:\n        List of child group dicts\n\n    Raises:\n        requests.HTTPError: If the API request fails\n\n    API Documentation:\n        https://ballchasing.com/doc/api#replay-groups-groups-get\n    \"\"\"\n    children = []\n    count = 200  # Max per page\n    after = None\n\n    while True:\n        self.rate_limiter.wait_if_needed()\n\n        params = {'group': parent_group_id, 'count': count}\n        if after:\n            params['after'] = after\n\n        url = f\"{self.base_url}/groups\"\n        response = self.session.get(url, params=params)\n        response.raise_for_status()\n        data = response.json()\n\n        batch = data.get('list', [])\n        children.extend(batch)\n\n        # Check for more pages\n        if 'next' not in data or len(batch) &lt; count:\n            break\n\n        after = batch[-1]['id'] if batch else None\n        time.sleep(0.5)  # Brief pause between pagination requests\n\n    return children\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.ballchasing_client.BallchasingClient.get_replays_from_group","title":"get_replays_from_group","text":"<pre><code>get_replays_from_group(group_id: str) -&gt; List[Dict]\n</code></pre> <p>Fetch all replay metadata from a specific group (non-recursive).</p> <p>Handles pagination automatically to fetch all replays.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Ballchasing group ID</p> required <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List of replay metadata dicts</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails</p> API Documentation <p>https://ballchasing.com/doc/api#replays-replays-get</p> Source code in <code>impulse/collection/ballchasing_client.py</code> <pre><code>def get_replays_from_group(self, group_id: str) -&gt; List[Dict]:\n    \"\"\"\n    Fetch all replay metadata from a specific group (non-recursive).\n\n    Handles pagination automatically to fetch all replays.\n\n    Args:\n        group_id: Ballchasing group ID\n\n    Returns:\n        List of replay metadata dicts\n\n    Raises:\n        requests.HTTPError: If the API request fails\n\n    API Documentation:\n        https://ballchasing.com/doc/api#replays-replays-get\n    \"\"\"\n    replays = []\n    count = 200  # Max per page\n    after = None\n\n    while True:\n        self.rate_limiter.wait_if_needed()\n\n        params = {'group': group_id, 'count': count}\n        if after:\n            params['after'] = after\n\n        url = f\"{self.base_url}/replays\"\n        response = self.session.get(url, params=params)\n        response.raise_for_status()\n        data = response.json()\n\n        batch = data.get('list', [])\n        replays.extend(batch)\n\n        # Check for more pages\n        if len(batch) &lt; count or 'next' not in data:\n            break\n\n        after = data.get('next')\n        time.sleep(0.5)  # Brief pause between pagination requests\n\n    return replays\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.ballchasing_client.BallchasingClient.download_replay_bytes","title":"download_replay_bytes","text":"<pre><code>download_replay_bytes(replay_id: str) -&gt; bytes\n</code></pre> <p>Download a replay file as raw bytes.</p> <p>Parameters:</p> Name Type Description Default <code>replay_id</code> <code>str</code> <p>Ballchasing replay ID</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>Raw replay file bytes</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails</p> API Documentation <p>https://ballchasing.com/doc/api#replays-replay-get-1</p> Source code in <code>impulse/collection/ballchasing_client.py</code> <pre><code>def download_replay_bytes(self, replay_id: str) -&gt; bytes:\n    \"\"\"\n    Download a replay file as raw bytes.\n\n    Args:\n        replay_id: Ballchasing replay ID\n\n    Returns:\n        Raw replay file bytes\n\n    Raises:\n        requests.HTTPError: If the API request fails\n\n    API Documentation:\n        https://ballchasing.com/doc/api#replays-replay-get-1\n    \"\"\"\n    self.rate_limiter.wait_if_needed()\n\n    url = f\"{self.base_url}/replays/{replay_id}/file\"\n    response = self.session.get(url)\n    response.raise_for_status()\n    return response.content\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.ballchasing_client.BallchasingClient.get_replay_metadata","title":"get_replay_metadata","text":"<pre><code>get_replay_metadata(replay_id: str) -&gt; Dict\n</code></pre> <p>Get metadata for a specific replay.</p> <p>Parameters:</p> Name Type Description Default <code>replay_id</code> <code>str</code> <p>Ballchasing replay ID</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dict with replay metadata</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If the API request fails</p> API Documentation <p>https://ballchasing.com/doc/api#replays-replay-get</p> Source code in <code>impulse/collection/ballchasing_client.py</code> <pre><code>def get_replay_metadata(self, replay_id: str) -&gt; Dict:\n    \"\"\"\n    Get metadata for a specific replay.\n\n    Args:\n        replay_id: Ballchasing replay ID\n\n    Returns:\n        Dict with replay metadata\n\n    Raises:\n        requests.HTTPError: If the API request fails\n\n    API Documentation:\n        https://ballchasing.com/doc/api#replays-replay-get\n    \"\"\"\n    self.rate_limiter.wait_if_needed()\n\n    url = f\"{self.base_url}/replays/{replay_id}\"\n    response = self.session.get(url)\n    response.raise_for_status()\n    return response.json()\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.ballchasing_client.BallchasingClient.build_group_tree","title":"build_group_tree","text":"<pre><code>build_group_tree(\n    group_id: str,\n    depth: int = 0,\n    progress_callback: Optional[callable] = None,\n) -&gt; Dict\n</code></pre> <p>Recursively build a tree structure of groups and replays.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Root group ID to start from</p> required <code>depth</code> <code>int</code> <p>Current recursion depth (used internally)</p> <code>0</code> <code>progress_callback</code> <code>Optional[callable]</code> <p>Optional callback function(message, depth) for progress updates</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dict with structure:</p> <code>Dict</code> <p>{ 'id': 'group-id', 'name': 'Group Name', 'children': [...],  # List of child group dicts 'replays': [...]    # List of replay dicts (only in leaf groups)</p> <code>Dict</code> <p>}</p> <p>Raises:</p> Type Description <code>HTTPError</code> <p>If any API request fails</p> Source code in <code>impulse/collection/ballchasing_client.py</code> <pre><code>def build_group_tree(self, group_id: str, depth: int = 0,\n                    progress_callback: Optional[callable] = None) -&gt; Dict:\n    \"\"\"\n    Recursively build a tree structure of groups and replays.\n\n    Args:\n        group_id: Root group ID to start from\n        depth: Current recursion depth (used internally)\n        progress_callback: Optional callback function(message, depth) for progress updates\n\n    Returns:\n        Dict with structure:\n        {\n            'id': 'group-id',\n            'name': 'Group Name',\n            'children': [...],  # List of child group dicts\n            'replays': [...]    # List of replay dicts (only in leaf groups)\n        }\n\n    Raises:\n        requests.HTTPError: If any API request fails\n    \"\"\"\n    # Get group info\n    group_info = self.get_group_info(group_id)\n    group_name = group_info.get('name', 'Unknown')\n\n    if progress_callback:\n        progress_callback(f\"Exploring: {group_name}\", depth)\n\n    tree = {\n        'id': group_id,\n        'name': group_name,\n        'children': [],\n        'replays': []\n    }\n\n    # Check for child groups\n    child_groups = self.get_child_groups(group_id)\n\n    if child_groups:\n        # Has subgroups - recurse\n        if progress_callback:\n            progress_callback(f\"Found {len(child_groups)} subgroups\", depth)\n\n        for child in child_groups:\n            child_id = child['id']\n            child_tree = self.build_group_tree(child_id, depth + 1, progress_callback)\n            tree['children'].append(child_tree)\n    else:\n        # Leaf group - fetch replays\n        if progress_callback:\n            progress_callback(\"Fetching replays...\", depth)\n\n        replays = self.get_replays_from_group(group_id)\n        tree['replays'] = replays\n\n        if progress_callback:\n            progress_callback(f\"Found {len(replays)} replays\", depth)\n\n    time.sleep(0.5)  # Brief pause between groups\n    return tree\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.ballchasing_client.BallchasingClient.get_rate_limit_status","title":"get_rate_limit_status","text":"<pre><code>get_rate_limit_status() -&gt; Dict\n</code></pre> <p>Get current rate limiter status.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>Dict with rate limit information:</p> <code>Dict</code> <p>{ 'requests_this_hour': int, 'requests_remaining': int, 'window_resets_in_seconds': float, 'window_resets_in_minutes': float</p> <code>Dict</code> <p>}</p> Source code in <code>impulse/collection/ballchasing_client.py</code> <pre><code>def get_rate_limit_status(self) -&gt; Dict:\n    \"\"\"\n    Get current rate limiter status.\n\n    Returns:\n        Dict with rate limit information:\n        {\n            'requests_this_hour': int,\n            'requests_remaining': int,\n            'window_resets_in_seconds': float,\n            'window_resets_in_minutes': float\n        }\n    \"\"\"\n    return self.rate_limiter.get_status()\n</code></pre>"},{"location":"collection/api-reference/#replaydownloader","title":"ReplayDownloader","text":""},{"location":"collection/api-reference/#impulse.collection.replay_downloader.ReplayDownloader","title":"ReplayDownloader","text":"<pre><code>ReplayDownloader(\n    client: BallchasingClient,\n    storage: StorageBackend,\n    db: Optional[ImpulseDB] = None,\n    progress_callback: Optional[\n        Callable[[DownloadProgress], None]\n    ] = None,\n)\n</code></pre> <p>High-level replay download orchestrator.</p> <p>Coordinates downloads from Ballchasing to any storage backend, with database tracking and progress reporting.</p> Example <p>from impulse.collection import ReplayDownloader, BallchasingClient from impulse.collection.storage import S3Backend from impulse.collection.database import ImpulseDB</p> <p>client = BallchasingClient() storage = S3Backend() db = ImpulseDB()</p> <p>downloader = ReplayDownloader(client, storage, db) result = downloader.download_group('rlcs-2024-abc123')</p> <p>Initialize replay downloader.</p> <p>Parameters:</p> Name Type Description Default <code>client</code> <code>BallchasingClient</code> <p>Ballchasing API client</p> required <code>storage</code> <code>StorageBackend</code> <p>Storage backend (S3, local, etc.)</p> required <code>db</code> <code>Optional[ImpulseDB]</code> <p>Optional database for tracking (enables deduplication and resume)</p> <code>None</code> <code>progress_callback</code> <code>Optional[Callable[[DownloadProgress], None]]</code> <p>Optional callback for progress updates</p> <code>None</code> Source code in <code>impulse/collection/replay_downloader.py</code> <pre><code>def __init__(\n    self,\n    client: BallchasingClient,\n    storage: StorageBackend,\n    db: Optional[ImpulseDB] = None,\n    progress_callback: Optional[Callable[[DownloadProgress], None]] = None\n):\n    \"\"\"\n    Initialize replay downloader.\n\n    Args:\n        client: Ballchasing API client\n        storage: Storage backend (S3, local, etc.)\n        db: Optional database for tracking (enables deduplication and resume)\n        progress_callback: Optional callback for progress updates\n    \"\"\"\n    self.client = client\n    self.storage = storage\n    self.db = db\n    self.progress_callback = progress_callback or self._default_progress_callback\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.replay_downloader.ReplayDownloader.download_group","title":"download_group","text":"<pre><code>download_group(\n    group_id: str,\n    path_prefix: Optional[List[str]] = None,\n    include_root_in_path: bool = True,\n    use_cache: bool = True,\n    only_replay_ids: Optional[List[str]] = None,\n) -&gt; DownloadResult\n</code></pre> <p>Download all replays from a Ballchasing group to storage.</p> <p>This method: 1. Builds the group tree (or loads from cache) 2. Registers replays in database (if enabled) 3. Downloads each replay from Ballchasing 4. Saves to storage backend 5. Updates database with completion status</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Ballchasing group ID</p> required <code>path_prefix</code> <code>Optional[List[str]]</code> <p>Optional prefix for storage paths (e.g., ['replays', 'rlcs'])</p> <code>None</code> <code>include_root_in_path</code> <code>bool</code> <p>Whether to include root group name in storage path</p> <code>True</code> <code>use_cache</code> <code>bool</code> <p>If True, load cached tree if available and save tree after building.        This saves API calls on retries. Default True.</p> <code>True</code> <code>only_replay_ids</code> <code>Optional[List[str]]</code> <p>If provided, only download these specific replay IDs.             Useful for retrying failed downloads.</p> <code>None</code> <p>Returns:</p> Type Description <code>DownloadResult</code> <p>DownloadResult with statistics</p> Example <p>result = downloader.download_group( ...     'rlcs-2024-abc123', ...     path_prefix=['replays', 'rlcs', '2024'] ... ) print(f\"Downloaded {result.successful}/{result.total_replays} replays\")</p> Source code in <code>impulse/collection/replay_downloader.py</code> <pre><code>def download_group(\n    self,\n    group_id: str,\n    path_prefix: Optional[List[str]] = None,\n    include_root_in_path: bool = True,\n    use_cache: bool = True,\n    only_replay_ids: Optional[List[str]] = None\n) -&gt; DownloadResult:\n    \"\"\"\n    Download all replays from a Ballchasing group to storage.\n\n    This method:\n    1. Builds the group tree (or loads from cache)\n    2. Registers replays in database (if enabled)\n    3. Downloads each replay from Ballchasing\n    4. Saves to storage backend\n    5. Updates database with completion status\n\n    Args:\n        group_id: Ballchasing group ID\n        path_prefix: Optional prefix for storage paths (e.g., ['replays', 'rlcs'])\n        include_root_in_path: Whether to include root group name in storage path\n        use_cache: If True, load cached tree if available and save tree after building.\n                   This saves API calls on retries. Default True.\n        only_replay_ids: If provided, only download these specific replay IDs.\n                        Useful for retrying failed downloads.\n\n    Returns:\n        DownloadResult with statistics\n\n    Example:\n        &gt;&gt;&gt; result = downloader.download_group(\n        ...     'rlcs-2024-abc123',\n        ...     path_prefix=['replays', 'rlcs', '2024']\n        ... )\n        &gt;&gt;&gt; print(f\"Downloaded {result.successful}/{result.total_replays} replays\")\n    \"\"\"\n    logger.info(f\"Starting download for group: {group_id}\")\n\n    # Try to load cached tree first\n    tree = None\n    if use_cache:\n        tree = load_group_tree(group_id)\n        if tree:\n            logger.info(\"Loaded group tree from cache\")\n            print(\"Using cached group tree (no API calls needed)\")\n\n    # Build group tree if not cached\n    if tree is None:\n        logger.info(\"Building group tree...\")\n\n        def tree_progress(message, depth):\n            indent = \"  \" * depth\n            print(f\"{indent}{message}\")\n\n        tree = self.client.build_group_tree(group_id, progress_callback=tree_progress)\n\n        # Save tree to cache for future retries\n        if use_cache:\n            cache_path = save_group_tree(tree, group_id)\n            logger.info(f\"Group tree cached at: {cache_path}\")\n            print(f\"Group tree cached for future retries\")\n\n    # Flatten to replay list\n    logger.info(\"Flattening tree structure...\")\n    replay_list = flatten_group_tree(tree)\n    total_in_tree = len(replay_list)\n    logger.info(f\"Found {total_in_tree} total replays in tree\")\n\n    # Filter to specific replay IDs if requested (for retries)\n    if only_replay_ids:\n        filter_set = set(only_replay_ids)\n        replay_list = [(r, p) for r, p in replay_list if r['id'] in filter_set]\n        logger.info(f\"Filtered to {len(replay_list)} replays for retry\")\n        print(f\"Retrying {len(replay_list)} specific replays\")\n\n    total_replays = len(replay_list)\n\n    # Register group and replays in database (skip if filtering for retry)\n    if self.db and not only_replay_ids:\n        self.db.register_group_download(tree['id'], tree['name'], total_in_tree)\n\n        logger.info(\"Registering replays in database...\")\n        new_count = 0\n        existing_count = 0\n\n        for replay, _ in replay_list:\n            is_new = self.db.add_replay(replay['id'], replay)\n            if is_new:\n                new_count += 1\n            else:\n                existing_count += 1\n\n        print(f\"  New replays: {new_count}\")\n        print(f\"  Already in database: {existing_count}\")\n\n    # Download replays\n    logger.info(\"Downloading replays...\")\n    print()\n    print(\"=\" * 60)\n    print(\"DOWNLOADING REPLAYS\")\n    print(\"=\" * 60)\n    print()\n\n    successful = 0\n    skipped = 0\n    failed = 0\n    total_bytes = 0\n    storage_keys = []\n    failed_replays = []\n\n    root_name = tree.get('name', group_id)\n\n    for i, (replay, group_path) in enumerate(replay_list, 1):\n        replay_id = replay['id']\n\n        # Build storage path components\n        if path_prefix:\n            components = path_prefix.copy()\n        else:\n            components = []\n\n        # Add group hierarchy to path\n        path_parts = build_path_components(\n            group_path,\n            root_name,\n            include_root=include_root_in_path\n        )\n        components.extend(path_parts)\n\n        # Get storage key for this replay\n        storage_key = self.storage.get_storage_key(replay_id, components)\n\n        # Check database first (resume capability)\n        if self.db and self.db.is_replay_downloaded(replay_id):\n            self.progress_callback(DownloadProgress(\n                current=i,\n                total=total_replays,\n                replay_id=replay_id,\n                status='skipped',\n                message='Already downloaded (database check)',\n                storage_key=storage_key\n            ))\n            skipped += 1\n            print()\n            continue\n\n        # Check storage directly (double-check)\n        if self.storage.replay_exists(replay_id, components):\n            size = self.storage.get_replay_size(replay_id, components)\n\n            # Update database if it was out of sync\n            if self.db:\n                self.db.mark_downloaded(replay_id, storage_key, size)\n\n            self.progress_callback(DownloadProgress(\n                current=i,\n                total=total_replays,\n                replay_id=replay_id,\n                status='skipped',\n                message='Already in storage (direct check)',\n                storage_key=storage_key\n            ))\n            skipped += 1\n            print()\n            continue\n\n        # Download and save replay\n        try:\n            # Download from Ballchasing\n            self.progress_callback(DownloadProgress(\n                current=i,\n                total=total_replays,\n                replay_id=replay_id,\n                status='downloading',\n                message='Downloading from Ballchasing...'\n            ))\n\n            replay_bytes = self.client.download_replay_bytes(replay_id)\n            file_size = len(replay_bytes)\n            file_size_mb = file_size / (1024 * 1024)\n\n            print(f\"  Downloaded: {file_size_mb:.2f} MB\")\n\n            # Prepare metadata\n            metadata = extract_replay_metadata(replay)\n            metadata['group_id'] = group_id\n\n            # Save to storage\n            self.progress_callback(DownloadProgress(\n                current=i,\n                total=total_replays,\n                replay_id=replay_id,\n                status='uploading',\n                message='Saving to storage...'\n            ))\n\n            save_result = self.storage.save_replay(\n                replay_id,\n                replay_bytes,\n                components,\n                metadata\n            )\n\n            if not save_result['success']:\n                raise Exception(save_result.get('error', 'Storage save failed'))\n\n            print(f\"  Saved to storage\")\n\n            # Update database\n            if self.db:\n                self.db.mark_downloaded(replay_id, storage_key, file_size)\n\n            successful += 1\n            total_bytes += file_size\n            storage_keys.append(storage_key)\n\n            self.progress_callback(DownloadProgress(\n                current=i,\n                total=total_replays,\n                replay_id=replay_id,\n                status='complete',\n                message='Complete',\n                storage_key=storage_key\n            ))\n\n            # Rate limit status every 50 replays\n            if i % 50 == 0:\n                status = self.client.get_rate_limit_status()\n                print(f\"\\nRate Limit Status:\")\n                print(f\"  Requests this hour: {status['requests_this_hour']}\")\n                print(f\"  Window resets in: {status['window_resets_in_minutes']:.1f} minutes\")\n                print()\n\n            print()\n\n        except Exception as e:\n            error_msg = str(e)\n            logger.error(f\"Failed to download {replay_id}: {error_msg}\")\n\n            self.progress_callback(DownloadProgress(\n                current=i,\n                total=total_replays,\n                replay_id=replay_id,\n                status='failed',\n                message='Failed',\n                error=error_msg\n            ))\n\n            if self.db:\n                self.db.mark_replay_failed(replay_id, error_msg)\n\n            failed += 1\n            failed_replays.append({'replay_id': replay_id, 'error': error_msg})\n            print()\n            continue\n\n    # Summary\n    print()\n    print(\"=\" * 60)\n    print(\"DOWNLOAD SUMMARY\")\n    print(\"=\" * 60)\n    print(f\"Total replays: {total_replays}\")\n    print(f\"Successfully downloaded: {successful}\")\n    print(f\"Skipped (already had): {skipped}\")\n    print(f\"Failed: {failed}\")\n    print(f\"Total size: {total_bytes / (1024**2):.2f} MB ({total_bytes / (1024**3):.2f} GB)\")\n\n    # Storage statistics\n    if path_prefix:\n        stats_prefix = path_prefix\n    else:\n        stats_prefix = [sanitize_path_component(root_name)]\n\n    storage_stats = self.storage.get_storage_stats(stats_prefix)\n    print()\n    print(\"STORAGE STATISTICS\")\n    print(\"-\" * 60)\n    print(f\"Total objects in storage: {storage_stats.get('total_replays', 0)}\")\n    print(f\"Total storage: {storage_stats.get('total_gb', 0):.2f} GB\")\n\n    # Database statistics\n    if self.db:\n        print()\n        print(\"DATABASE STATISTICS\")\n        print(\"-\" * 60)\n        db_stats = self.db.get_stats()\n        for key, value in db_stats.items():\n            print(f\"{key}: {value}\")\n\n    return DownloadResult(\n        total_replays=total_replays,\n        successful=successful,\n        skipped=skipped,\n        failed=failed,\n        total_bytes=total_bytes,\n        storage_keys=storage_keys,\n        failed_replays=failed_replays\n    )\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.replay_downloader.ReplayDownloader.retry_failed_downloads","title":"retry_failed_downloads","text":"<pre><code>retry_failed_downloads(\n    group_id: str,\n    failed_replays: List[Dict],\n    path_prefix: Optional[List[str]] = None,\n    include_root_in_path: bool = True,\n) -&gt; DownloadResult\n</code></pre> <p>Retry downloading failed replays using the cached group tree.</p> <p>A convenience wrapper around download_group() that accepts the failed_replays list from a previous DownloadResult.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Ballchasing group ID (must have a cached tree)</p> required <code>failed_replays</code> <code>List[Dict]</code> <p>The failed_replays list from a previous DownloadResult            (list of dicts with 'replay_id' key)</p> required <code>path_prefix</code> <code>Optional[List[str]]</code> <p>Optional prefix for storage paths</p> <code>None</code> <code>include_root_in_path</code> <code>bool</code> <p>Whether to include root group name in storage path</p> <code>True</code> <p>Returns:</p> Type Description <code>DownloadResult</code> <p>DownloadResult with statistics for the retry attempt</p> Example <p>result = downloader.download_group('rlcs-2024-abc123') if result.failed_replays: ...     retry_result = downloader.retry_failed_downloads( ...         'rlcs-2024-abc123', ...         result.failed_replays ...     )</p> Source code in <code>impulse/collection/replay_downloader.py</code> <pre><code>def retry_failed_downloads(\n    self,\n    group_id: str,\n    failed_replays: List[Dict],\n    path_prefix: Optional[List[str]] = None,\n    include_root_in_path: bool = True\n) -&gt; DownloadResult:\n    \"\"\"\n    Retry downloading failed replays using the cached group tree.\n\n    A convenience wrapper around download_group() that accepts the\n    failed_replays list from a previous DownloadResult.\n\n    Args:\n        group_id: Ballchasing group ID (must have a cached tree)\n        failed_replays: The failed_replays list from a previous DownloadResult\n                       (list of dicts with 'replay_id' key)\n        path_prefix: Optional prefix for storage paths\n        include_root_in_path: Whether to include root group name in storage path\n\n    Returns:\n        DownloadResult with statistics for the retry attempt\n\n    Example:\n        &gt;&gt;&gt; result = downloader.download_group('rlcs-2024-abc123')\n        &gt;&gt;&gt; if result.failed_replays:\n        ...     retry_result = downloader.retry_failed_downloads(\n        ...         'rlcs-2024-abc123',\n        ...         result.failed_replays\n        ...     )\n    \"\"\"\n    replay_ids = [r['replay_id'] for r in failed_replays]\n    return self.download_group(\n        group_id=group_id,\n        path_prefix=path_prefix,\n        include_root_in_path=include_root_in_path,\n        use_cache=True,\n        only_replay_ids=replay_ids\n    )\n</code></pre>"},{"location":"collection/api-reference/#storage","title":"Storage","text":""},{"location":"collection/api-reference/#impulse.collection.storage.StorageBackend","title":"StorageBackend","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for storage backends.</p> <p>All storage backends must implement these methods to provide a consistent interface for saving, retrieving, and managing replay files.</p>"},{"location":"collection/api-reference/#impulse.collection.storage.StorageBackend.save_replay","title":"save_replay  <code>abstractmethod</code>","text":"<pre><code>save_replay(\n    replay_id: str,\n    data: bytes,\n    path_components: List[str],\n    metadata: Optional[Dict] = None,\n) -&gt; Dict\n</code></pre> <p>Save a replay file to storage.</p> <p>Parameters:</p> Name Type Description Default <code>replay_id</code> <code>str</code> <p>Unique replay identifier</p> required <code>data</code> <code>bytes</code> <p>Raw replay file bytes</p> required <code>path_components</code> <code>List[str]</code> <p>List of path components for hierarchical organization            e.g., ['replays', 'rlcs', '2024', 'worlds']</p> required <code>metadata</code> <code>Optional[Dict]</code> <p>Optional metadata to attach to the file</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dict with 'success' (bool), 'storage_key' (str), 'size_bytes' (int)</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>@abstractmethod\ndef save_replay(self, replay_id: str, data: bytes, path_components: List[str],\n                metadata: Optional[Dict] = None) -&gt; Dict:\n    \"\"\"\n    Save a replay file to storage.\n\n    Args:\n        replay_id: Unique replay identifier\n        data: Raw replay file bytes\n        path_components: List of path components for hierarchical organization\n                       e.g., ['replays', 'rlcs', '2024', 'worlds']\n        metadata: Optional metadata to attach to the file\n\n    Returns:\n        Dict with 'success' (bool), 'storage_key' (str), 'size_bytes' (int)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.StorageBackend.replay_exists","title":"replay_exists  <code>abstractmethod</code>","text":"<pre><code>replay_exists(\n    replay_id: str, path_components: List[str]\n) -&gt; bool\n</code></pre> <p>Check if a replay file exists in storage.</p> <p>Parameters:</p> Name Type Description Default <code>replay_id</code> <code>str</code> <p>Unique replay identifier</p> required <code>path_components</code> <code>List[str]</code> <p>Path components where replay should be located</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if replay exists</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>@abstractmethod\ndef replay_exists(self, replay_id: str, path_components: List[str]) -&gt; bool:\n    \"\"\"\n    Check if a replay file exists in storage.\n\n    Args:\n        replay_id: Unique replay identifier\n        path_components: Path components where replay should be located\n\n    Returns:\n        True if replay exists\n    \"\"\"\n    pass\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.StorageBackend.get_replay_size","title":"get_replay_size  <code>abstractmethod</code>","text":"<pre><code>get_replay_size(\n    replay_id: str, path_components: List[str]\n) -&gt; int\n</code></pre> <p>Get the size of a stored replay file in bytes.</p> <p>Parameters:</p> Name Type Description Default <code>replay_id</code> <code>str</code> <p>Unique replay identifier</p> required <code>path_components</code> <code>List[str]</code> <p>Path components where replay is located</p> required <p>Returns:</p> Type Description <code>int</code> <p>Size in bytes, or 0 if not found</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>@abstractmethod\ndef get_replay_size(self, replay_id: str, path_components: List[str]) -&gt; int:\n    \"\"\"\n    Get the size of a stored replay file in bytes.\n\n    Args:\n        replay_id: Unique replay identifier\n        path_components: Path components where replay is located\n\n    Returns:\n        Size in bytes, or 0 if not found\n    \"\"\"\n    pass\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.StorageBackend.list_replays","title":"list_replays  <code>abstractmethod</code>","text":"<pre><code>list_replays(path_prefix: List[str]) -&gt; List[str]\n</code></pre> <p>List all replay IDs under a given path prefix.</p> <p>Parameters:</p> Name Type Description Default <code>path_prefix</code> <code>List[str]</code> <p>Path components to search under</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of replay IDs</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>@abstractmethod\ndef list_replays(self, path_prefix: List[str]) -&gt; List[str]:\n    \"\"\"\n    List all replay IDs under a given path prefix.\n\n    Args:\n        path_prefix: Path components to search under\n\n    Returns:\n        List of replay IDs\n    \"\"\"\n    pass\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.StorageBackend.get_storage_stats","title":"get_storage_stats  <code>abstractmethod</code>","text":"<pre><code>get_storage_stats(path_prefix: List[str]) -&gt; Dict\n</code></pre> <p>Get storage statistics for replays under a path prefix.</p> <p>Parameters:</p> Name Type Description Default <code>path_prefix</code> <code>List[str]</code> <p>Path components to calculate stats for</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Dict with 'total_replays', 'total_bytes', 'total_mb', 'total_gb'</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>@abstractmethod\ndef get_storage_stats(self, path_prefix: List[str]) -&gt; Dict:\n    \"\"\"\n    Get storage statistics for replays under a path prefix.\n\n    Args:\n        path_prefix: Path components to calculate stats for\n\n    Returns:\n        Dict with 'total_replays', 'total_bytes', 'total_mb', 'total_gb'\n    \"\"\"\n    pass\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.StorageBackend.get_storage_key","title":"get_storage_key  <code>abstractmethod</code>","text":"<pre><code>get_storage_key(\n    replay_id: str, path_components: List[str]\n) -&gt; str\n</code></pre> <p>Get the full storage key/path for a replay.</p> <p>Parameters:</p> Name Type Description Default <code>replay_id</code> <code>str</code> <p>Unique replay identifier</p> required <code>path_components</code> <code>List[str]</code> <p>Path components</p> required <p>Returns:</p> Type Description <code>str</code> <p>Full storage key (S3 key, file path, etc.)</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>@abstractmethod\ndef get_storage_key(self, replay_id: str, path_components: List[str]) -&gt; str:\n    \"\"\"\n    Get the full storage key/path for a replay.\n\n    Args:\n        replay_id: Unique replay identifier\n        path_components: Path components\n\n    Returns:\n        Full storage key (S3 key, file path, etc.)\n    \"\"\"\n    pass\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.LocalBackend","title":"LocalBackend","text":"<pre><code>LocalBackend(base_dir: str = './replays/raw')\n</code></pre> <p>               Bases: <code>StorageBackend</code></p> <p>Local filesystem storage backend.</p> <p>Stores replay files in a local directory structure.</p> <p>Initialize local storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>base_dir</code> <code>str</code> <p>Base directory for storing replays</p> <code>'./replays/raw'</code> Source code in <code>impulse/collection/storage.py</code> <pre><code>def __init__(self, base_dir: str = \"./replays/raw\"):\n    \"\"\"\n    Initialize local storage backend.\n\n    Args:\n        base_dir: Base directory for storing replays\n    \"\"\"\n    self.base_dir = Path(base_dir)\n    self.base_dir.mkdir(parents=True, exist_ok=True)\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.LocalBackend.save_replay","title":"save_replay","text":"<pre><code>save_replay(\n    replay_id: str,\n    data: bytes,\n    path_components: List[str],\n    metadata: Optional[Dict] = None,\n) -&gt; Dict\n</code></pre> <p>Save replay to local filesystem.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def save_replay(self, replay_id: str, data: bytes, path_components: List[str],\n               metadata: Optional[Dict] = None) -&gt; Dict:\n    \"\"\"Save replay to local filesystem.\"\"\"\n    try:\n        # Build full path\n        replay_dir = self.base_dir / Path(*path_components)\n        replay_dir.mkdir(parents=True, exist_ok=True)\n\n        filepath = replay_dir / f\"{replay_id}.replay\"\n\n        # Write file\n        filepath.write_bytes(data)\n\n        # Optionally save metadata as JSON sidecar\n        if metadata:\n            import json\n            metadata_path = replay_dir / f\"{replay_id}.metadata.json\"\n            metadata_path.write_text(json.dumps(metadata, indent=2))\n\n        return {\n            'success': True,\n            'storage_key': str(filepath.relative_to(self.base_dir)),\n            'size_bytes': len(data),\n            'full_path': str(filepath)\n        }\n\n    except Exception as e:\n        return {\n            'success': False,\n            'error': str(e),\n            'storage_key': None,\n            'size_bytes': 0\n        }\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.LocalBackend.replay_exists","title":"replay_exists","text":"<pre><code>replay_exists(\n    replay_id: str, path_components: List[str]\n) -&gt; bool\n</code></pre> <p>Check if replay exists locally.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def replay_exists(self, replay_id: str, path_components: List[str]) -&gt; bool:\n    \"\"\"Check if replay exists locally.\"\"\"\n    filepath = self.base_dir / Path(*path_components) / f\"{replay_id}.replay\"\n    return filepath.exists()\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.LocalBackend.get_replay_size","title":"get_replay_size","text":"<pre><code>get_replay_size(\n    replay_id: str, path_components: List[str]\n) -&gt; int\n</code></pre> <p>Get replay file size.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def get_replay_size(self, replay_id: str, path_components: List[str]) -&gt; int:\n    \"\"\"Get replay file size.\"\"\"\n    filepath = self.base_dir / Path(*path_components) / f\"{replay_id}.replay\"\n    if filepath.exists():\n        return filepath.stat().st_size\n    return 0\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.LocalBackend.list_replays","title":"list_replays","text":"<pre><code>list_replays(path_prefix: List[str]) -&gt; List[str]\n</code></pre> <p>List all replay IDs under a path prefix.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def list_replays(self, path_prefix: List[str]) -&gt; List[str]:\n    \"\"\"List all replay IDs under a path prefix.\"\"\"\n    search_dir = self.base_dir / Path(*path_prefix)\n    if not search_dir.exists():\n        return []\n\n    replay_ids = []\n    for replay_file in search_dir.rglob(\"*.replay\"):\n        # Extract replay ID (filename without extension)\n        replay_ids.append(replay_file.stem)\n\n    return replay_ids\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.LocalBackend.get_storage_stats","title":"get_storage_stats","text":"<pre><code>get_storage_stats(path_prefix: List[str]) -&gt; Dict\n</code></pre> <p>Get storage statistics.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def get_storage_stats(self, path_prefix: List[str]) -&gt; Dict:\n    \"\"\"Get storage statistics.\"\"\"\n    search_dir = self.base_dir / Path(*path_prefix)\n    if not search_dir.exists():\n        return {\n            'total_replays': 0,\n            'total_bytes': 0,\n            'total_mb': 0.0,\n            'total_gb': 0.0\n        }\n\n    total_bytes = 0\n    total_replays = 0\n\n    for replay_file in search_dir.rglob(\"*.replay\"):\n        total_bytes += replay_file.stat().st_size\n        total_replays += 1\n\n    return {\n        'total_replays': total_replays,\n        'total_bytes': total_bytes,\n        'total_mb': round(total_bytes / (1024**2), 2),\n        'total_gb': round(total_bytes / (1024**3), 2)\n    }\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.LocalBackend.get_storage_key","title":"get_storage_key","text":"<pre><code>get_storage_key(\n    replay_id: str, path_components: List[str]\n) -&gt; str\n</code></pre> <p>Get full file path.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def get_storage_key(self, replay_id: str, path_components: List[str]) -&gt; str:\n    \"\"\"Get full file path.\"\"\"\n    filepath = self.base_dir / Path(*path_components) / f\"{replay_id}.replay\"\n    return str(filepath.relative_to(self.base_dir))\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.S3Backend","title":"S3Backend","text":"<pre><code>S3Backend(\n    s3_manager: S3Manager = None,\n    aws_region: str = None,\n    s3_bucket_name: str = None,\n)\n</code></pre> <p>               Bases: <code>StorageBackend</code></p> <p>AWS S3 storage backend.</p> <p>Wraps the existing S3Manager to provide the StorageBackend interface.</p> <p>Initialize S3 storage backend.</p> <p>Parameters:</p> Name Type Description Default <code>s3_manager</code> <code>S3Manager</code> <p>Optional existing S3Manager instance (for dependency injection)</p> <code>None</code> <code>aws_region</code> <code>str</code> <p>AWS region (defaults to env var, ignored if s3_manager provided)</p> <code>None</code> <code>s3_bucket_name</code> <code>str</code> <p>S3 bucket name (defaults to env var, ignored if s3_manager provided)</p> <code>None</code> Source code in <code>impulse/collection/storage.py</code> <pre><code>def __init__(self, s3_manager: S3Manager = None, aws_region: str = None,\n             s3_bucket_name: str = None):\n    \"\"\"\n    Initialize S3 storage backend.\n\n    Args:\n        s3_manager: Optional existing S3Manager instance (for dependency injection)\n        aws_region: AWS region (defaults to env var, ignored if s3_manager provided)\n        s3_bucket_name: S3 bucket name (defaults to env var, ignored if s3_manager provided)\n    \"\"\"\n    if s3_manager:\n        self.s3_manager = s3_manager\n    else:\n        self.s3_manager = S3Manager(aws_region, s3_bucket_name)\n        # Ensure bucket exists\n        self.s3_manager.create_bucket_if_needed()\n\n    self.bucket_name = self.s3_manager.s3_bucket_name\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.S3Backend.save_replay","title":"save_replay","text":"<pre><code>save_replay(\n    replay_id: str,\n    data: bytes,\n    path_components: List[str],\n    metadata: Optional[Dict] = None,\n) -&gt; Dict\n</code></pre> <p>Save replay to S3.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def save_replay(self, replay_id: str, data: bytes, path_components: List[str],\n               metadata: Optional[Dict] = None) -&gt; Dict:\n    \"\"\"Save replay to S3.\"\"\"\n    # Build S3 key from path components\n    s3_key = '/'.join(path_components) + f'/{replay_id}.replay'\n\n    # Upload to S3\n    result = self.s3_manager.upload_bytes(data, s3_key, metadata)\n\n    return result\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.S3Backend.replay_exists","title":"replay_exists","text":"<pre><code>replay_exists(\n    replay_id: str, path_components: List[str]\n) -&gt; bool\n</code></pre> <p>Check if replay exists in S3.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def replay_exists(self, replay_id: str, path_components: List[str]) -&gt; bool:\n    \"\"\"Check if replay exists in S3.\"\"\"\n    s3_key = '/'.join(path_components) + f'/{replay_id}.replay'\n    return self.s3_manager.object_exists(s3_key)\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.S3Backend.get_replay_size","title":"get_replay_size","text":"<pre><code>get_replay_size(\n    replay_id: str, path_components: List[str]\n) -&gt; int\n</code></pre> <p>Get replay file size from S3.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def get_replay_size(self, replay_id: str, path_components: List[str]) -&gt; int:\n    \"\"\"Get replay file size from S3.\"\"\"\n    s3_key = '/'.join(path_components) + f'/{replay_id}.replay'\n    return self.s3_manager.get_object_size(s3_key)\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.S3Backend.list_replays","title":"list_replays","text":"<pre><code>list_replays(path_prefix: List[str]) -&gt; List[str]\n</code></pre> <p>List all replay IDs under a path prefix in S3.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def list_replays(self, path_prefix: List[str]) -&gt; List[str]:\n    \"\"\"List all replay IDs under a path prefix in S3.\"\"\"\n    prefix = '/'.join(path_prefix) + '/' if path_prefix else ''\n\n    # Get all objects with this prefix\n    objects = self.s3_manager.list_objects(prefix, max_keys=10000)\n\n    # Extract replay IDs\n    replay_ids = []\n    for obj_key in objects:\n        if obj_key.endswith('.replay'):\n            # Extract filename\n            filename = obj_key.split('/')[-1]\n            replay_id = filename.replace('.replay', '')\n            replay_ids.append(replay_id)\n\n    return replay_ids\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.S3Backend.get_storage_stats","title":"get_storage_stats","text":"<pre><code>get_storage_stats(path_prefix: List[str]) -&gt; Dict\n</code></pre> <p>Get S3 storage statistics.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def get_storage_stats(self, path_prefix: List[str]) -&gt; Dict:\n    \"\"\"Get S3 storage statistics.\"\"\"\n    prefix = '/'.join(path_prefix) + '/' if path_prefix else ''\n    return self.s3_manager.get_storage_stats(prefix)\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.S3Backend.get_storage_key","title":"get_storage_key","text":"<pre><code>get_storage_key(\n    replay_id: str, path_components: List[str]\n) -&gt; str\n</code></pre> <p>Get S3 key.</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def get_storage_key(self, replay_id: str, path_components: List[str]) -&gt; str:\n    \"\"\"Get S3 key.\"\"\"\n    return '/'.join(path_components) + f'/{replay_id}.replay'\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.storage.S3Backend.backup_database","title":"backup_database","text":"<pre><code>backup_database(\n    db_path: str, backup_prefix: str = \"database-backups\"\n) -&gt; Dict\n</code></pre> <p>Convenience method to backup database to S3.</p> <p>Parameters:</p> Name Type Description Default <code>db_path</code> <code>str</code> <p>Path to local database file</p> required <code>backup_prefix</code> <code>str</code> <p>S3 prefix for backups</p> <code>'database-backups'</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dict with backup info</p> Source code in <code>impulse/collection/storage.py</code> <pre><code>def backup_database(self, db_path: str, backup_prefix: str = \"database-backups\") -&gt; Dict:\n    \"\"\"\n    Convenience method to backup database to S3.\n\n    Args:\n        db_path: Path to local database file\n        backup_prefix: S3 prefix for backups\n\n    Returns:\n        Dict with backup info\n    \"\"\"\n    return self.s3_manager.backup_database(db_path, backup_prefix)\n</code></pre>"},{"location":"collection/api-reference/#rlcsmanager","title":"RLCSManager","text":""},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager","title":"RLCSManager","text":"<pre><code>RLCSManager(\n    storage_type: str = \"s3\",\n    path_prefix: Optional[List[str]] = None,\n    output_dir: Optional[str] = None,\n    use_database: bool = True,\n)\n</code></pre> <p>Manager for RLCS season downloads and metadata.</p> <p>Provides a unified interface for downloading complete RLCS seasons to local or S3 storage, with support for dry-run mode, progress tracking, and logging.</p> Usage <p>Initialize RLCS manager.</p> <p>Parameters:</p> Name Type Description Default <code>storage_type</code> <code>str</code> <p>Storage backend ('s3' or 'local')</p> <code>'s3'</code> <code>path_prefix</code> <code>Optional[List[str]]</code> <p>S3 path prefix (default: ['replays', 'rlcs'])</p> <code>None</code> <code>output_dir</code> <code>Optional[str]</code> <p>Local output directory (required if storage_type='local')</p> <code>None</code> <code>use_database</code> <code>bool</code> <p>Whether to use database for tracking (default: True)</p> <code>True</code> Source code in <code>impulse/collection/rlcs_manager.py</code> <pre><code>def __init__(\n    self,\n    storage_type: str = 's3',\n    path_prefix: Optional[List[str]] = None,\n    output_dir: Optional[str] = None,\n    use_database: bool = True\n):\n    \"\"\"\n    Initialize RLCS manager.\n\n    Args:\n        storage_type: Storage backend ('s3' or 'local')\n        path_prefix: S3 path prefix (default: ['replays', 'rlcs'])\n        output_dir: Local output directory (required if storage_type='local')\n        use_database: Whether to use database for tracking (default: True)\n    \"\"\"\n    if storage_type not in ['s3', 'local']:\n        raise ValueError(f\"storage_type must be 's3' or 'local', got: {storage_type}\")\n\n    if storage_type == 'local' and not output_dir:\n        raise ValueError(\"output_dir is required when storage_type='local'\")\n\n    self.storage_type = storage_type\n    self.path_prefix = path_prefix or ['replays', 'rlcs']\n    self.output_dir = output_dir\n    self.use_database = use_database\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager--download-to-s3","title":"Download to S3","text":"<p>rlcs = RLCSManager(storage_type='s3') rlcs.download_season('2024')</p>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager--download-to-local-storage","title":"Download to local storage","text":"<p>rlcs = RLCSManager(storage_type='local', output_dir='./replays') rlcs.download_season('2024')</p>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager--list-available-seasons","title":"List available seasons","text":"<p>rlcs.list_seasons()</p>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager--get-season-info-without-downloading","title":"Get season info without downloading","text":"<p>info = RLCSManager.get_season_info('2024')</p>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager.get_season_info","title":"get_season_info  <code>classmethod</code>","text":"<pre><code>get_season_info(season_key: str) -&gt; Dict[str, Any]\n</code></pre> <p>Get metadata for a specific season.</p> <p>Parameters:</p> Name Type Description Default <code>season_key</code> <code>str</code> <p>Season identifier (e.g., '2024', '21-22')</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary with season metadata</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If season not found</p> Source code in <code>impulse/collection/rlcs_manager.py</code> <pre><code>@classmethod\ndef get_season_info(cls, season_key: str) -&gt; Dict[str, Any]:\n    \"\"\"\n    Get metadata for a specific season.\n\n    Args:\n        season_key: Season identifier (e.g., '2024', '21-22')\n\n    Returns:\n        Dictionary with season metadata\n\n    Raises:\n        KeyError: If season not found\n    \"\"\"\n    if season_key not in cls.SEASONS:\n        available = ', '.join(cls.SEASONS.keys())\n        raise KeyError(f\"Season '{season_key}' not found. Available: {available}\")\n\n    return cls.SEASONS[season_key]\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager.get_available_seasons","title":"get_available_seasons  <code>classmethod</code>","text":"<pre><code>get_available_seasons() -&gt; List[str]\n</code></pre> <p>Get list of available season keys.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of season identifiers</p> Source code in <code>impulse/collection/rlcs_manager.py</code> <pre><code>@classmethod\ndef get_available_seasons(cls) -&gt; List[str]:\n    \"\"\"\n    Get list of available season keys.\n\n    Returns:\n        List of season identifiers\n    \"\"\"\n    return list(cls.SEASONS.keys())\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager.print_season_info","title":"print_season_info","text":"<pre><code>print_season_info(season_key: str) -&gt; None\n</code></pre> <p>Print detailed information about a season.</p> <p>Parameters:</p> Name Type Description Default <code>season_key</code> <code>str</code> <p>Season identifier (e.g., '2024')</p> required Source code in <code>impulse/collection/rlcs_manager.py</code> <pre><code>def print_season_info(self, season_key: str) -&gt; None:\n    \"\"\"\n    Print detailed information about a season.\n\n    Args:\n        season_key: Season identifier (e.g., '2024')\n    \"\"\"\n    season = self.get_season_info(season_key)\n\n    print(\"=\" * 60)\n    print(f\"RLCS {season_key} Season Download\")\n    print(\"=\" * 60)\n    print(f\"Season Name: {season['name']}\")\n    print(f\"Group ID: {season['group_id']}\")\n    print(f\"Estimated Replays: {season['estimated_replay_count']:,}\")\n    print(f\"Estimated Size: {season['estimated_size_gb']:.1f} GB\")\n    print(f\"Active Season: {season['is_active']} (as of {season['last_updated']})\")\n    print()\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager.list_seasons","title":"list_seasons","text":"<pre><code>list_seasons() -&gt; None\n</code></pre> <p>Print information about all available seasons.</p> Source code in <code>impulse/collection/rlcs_manager.py</code> <pre><code>def list_seasons(self) -&gt; None:\n    \"\"\"Print information about all available seasons.\"\"\"\n    print()\n    print(\"=\" * 60)\n    print(\"Available RLCS Seasons:\")\n    for season_key, season_data in self.SEASONS.items():\n        print(f\"\\n  Season Key: {season_key}\")\n        print(f\"  Season Name: {season_data['name']}\")\n        print(f\"  Group ID: {season_data['group_id']}\")\n        print(f\"  Estimated replay count: {season_data['estimated_replay_count']:,} replays\")\n        print(f\"  Estimated total download size: {season_data['estimated_size_gb']:.1f} GB\")\n        print(f\"  Active Season: {season_data['is_active']} (as of {season_data['last_updated']})\")\n    print(\"\\nUse download_season(season_key) to download a specific season.\")\n    print()\n    print(\"=\" * 60)\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.rlcs_manager.RLCSManager.download_season","title":"download_season","text":"<pre><code>download_season(\n    season_key: str,\n    dry_run: bool = False,\n    confirm: bool = True,\n    storage_type: Optional[str] = None,\n    output_dir: Optional[str] = None,\n) -&gt; Optional[Any]\n</code></pre> <p>Download a complete RLCS season.</p> <p>Parameters:</p> Name Type Description Default <code>season_key</code> <code>str</code> <p>Season identifier (e.g., '2024')</p> required <code>dry_run</code> <code>bool</code> <p>If True, preview without downloading</p> <code>False</code> <code>confirm</code> <code>bool</code> <p>If True, prompt for user confirmation (default: True)</p> <code>True</code> <code>storage_type</code> <code>Optional[str]</code> <p>Override instance storage_type ('s3' or 'local')</p> <code>None</code> <code>output_dir</code> <code>Optional[str]</code> <p>Override instance output_dir (for local storage)</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Any]</code> <p>Download result object if successful, None if cancelled/dry-run</p> Source code in <code>impulse/collection/rlcs_manager.py</code> <pre><code>def download_season(\n    self,\n    season_key: str,\n    dry_run: bool = False,\n    confirm: bool = True,\n    storage_type: Optional[str] = None,\n    output_dir: Optional[str] = None\n) -&gt; Optional[Any]:\n    \"\"\"\n    Download a complete RLCS season.\n\n    Args:\n        season_key: Season identifier (e.g., '2024')\n        dry_run: If True, preview without downloading\n        confirm: If True, prompt for user confirmation (default: True)\n        storage_type: Override instance storage_type ('s3' or 'local')\n        output_dir: Override instance output_dir (for local storage)\n\n    Returns:\n        Download result object if successful, None if cancelled/dry-run\n    \"\"\"\n    # Validate season\n    try:\n        season = self.get_season_info(season_key)\n    except KeyError as e:\n        print(f\"\u2717 {e}\")\n        return None\n\n    # Determine storage configuration\n    storage = storage_type or self.storage_type\n    out_dir = output_dir or self.output_dir\n\n    if storage == 'local' and not out_dir:\n        raise ValueError(\"output_dir is required for local storage\")\n\n    # Print season info\n    self.print_season_info(season_key)\n\n    if dry_run:\n        print(\"DRY RUN MODE - No actual download\")\n        print(f\"Storage: {storage}\")\n        if storage == 'local':\n            print(f\"Output directory: {out_dir}\")\n        else:\n            print(f\"S3 path prefix: {'/'.join(self.path_prefix + [season_key])}\")\n        return None\n\n    # Confirm with user\n    if confirm:\n        print(f\"WARNING: This will download {season['estimated_replay_count']} replays to {storage} storage.\"\n              f\"Estimated download size: {season['estimated_size_gb']:.1f} GB!\")\n        if storage == 's3':\n            print(\"Make sure you're running on EC2 (not locally)\")\n        print()\n        response = input(\"Continue? (yes/no): \")\n\n        if response.lower() not in ['yes', 'y']:\n            print(\"Download cancelled\")\n            return None\n\n    # Log start time\n    start_time = datetime.now(timezone.utc)\n    print(f\"\\nStarted: {start_time.isoformat()}\")\n    print()\n\n    try:\n        # Import here to avoid circular dependency\n        from impulse.collection import download_group\n\n        # Download based on storage type\n        if storage == 'local':\n            result = download_group(\n                group_id=season['group_id'],\n                storage_type='local',\n                output_dir=out_dir,\n                use_database=self.use_database\n            )\n        else:  # s3\n            path_prefix = self.path_prefix + [season_key]\n            result = download_group(\n                group_id=season['group_id'],\n                storage_type='s3',\n                path_prefix=path_prefix,\n                use_database=self.use_database\n            )\n\n        # Log completion\n        end_time = datetime.now(timezone.utc)\n        duration = end_time - start_time\n\n        self._print_completion_summary(start_time, end_time, duration, result)\n\n        # Save completion log\n        log_file = self._save_completion_log(\n            season_key, season, start_time, end_time, duration, result\n        )\n\n        # Upload log to S3 if using S3 storage\n        if storage == 's3':\n            try:\n                s3_backend = S3Backend()\n                s3_backend.s3_manager.upload_file(log_file, f\"logs/{log_file}\")\n                print(f\"Log backed up to S3\")\n            except Exception as e:\n                print(f\"Warning: Could not upload log to S3: {e}\")\n\n        return result\n\n    except KeyboardInterrupt:\n        print(\"\\n\\n   Download interrupted by user\")\n        print(\"Run the same command again to resume (database tracks progress)\")\n        return None\n    except Exception as e:\n        print(f\"\\n\\n Download failed: {e}\")\n        import traceback\n        traceback.print_exc()\n        return None\n</code></pre>"},{"location":"collection/api-reference/#impulsedb","title":"ImpulseDB","text":""},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB","title":"ImpulseDB","text":"<pre><code>ImpulseDB(db_path: str = './impulse.db')\n</code></pre> <p>Manages SQLite database for replay tracking (raw and parsed).</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def __init__(self, db_path: str = \"./impulse.db\"):\n    self.db_path = Path(db_path)\n    self.db_path.parent.mkdir(parents=True, exist_ok=True)\n    self.init_database()\n    print(f\"Database initialized: {self.db_path}\")\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_connection","title":"get_connection","text":"<pre><code>get_connection()\n</code></pre> <p>Context manager for database connections.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>@contextmanager\ndef get_connection(self):\n    \"\"\"Context manager for database connections.\"\"\"\n    conn = sqlite3.connect(self.db_path)\n    conn.row_factory = sqlite3.Row\n    try:\n        yield conn\n        conn.commit()\n    except Exception as e:\n        conn.rollback()\n        raise e\n    finally:\n        conn.close()\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.init_database","title":"init_database","text":"<pre><code>init_database()\n</code></pre> <p>Create all tables if they don't exist.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def init_database(self):\n    \"\"\"Create all tables if they don't exist.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n\n        # Table 1: Groups - track Ballchasing groups we've synced\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS groups (\n                group_id TEXT PRIMARY KEY,\n                name TEXT NOT NULL,\n                downloaded_at TEXT,\n                replay_count INTEGER DEFAULT 0\n            )\n        \"\"\")\n\n        # Table 2: Raw Replays - tracks downloaded .replay files\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS raw_replays (\n                replay_id TEXT PRIMARY KEY,\n                title TEXT,\n                date TEXT,\n                blue_team TEXT,\n                orange_team TEXT,\n                storage_key TEXT,\n                file_size_bytes INTEGER,\n                downloaded_at TEXT,\n                is_downloaded BOOLEAN DEFAULT 0,\n                download_status TEXT DEFAULT 'pending',\n                error_message TEXT,\n                CHECK (download_status IN ('pending', 'downloaded', 'failed'))\n            )\n        \"\"\")\n\n        # Table 3: Parsed Replays - tracks parsed replay data\n        cursor.execute(\"\"\"\n            CREATE TABLE IF NOT EXISTS parsed_replays (\n                replay_id TEXT PRIMARY KEY,\n                raw_replay_id TEXT,\n                output_path TEXT,\n                output_format TEXT,\n                fps REAL,\n                frame_count INTEGER,\n                feature_count INTEGER,\n                file_size_bytes INTEGER,\n                parsed_at TEXT,\n                parse_status TEXT DEFAULT 'pending',\n                error_message TEXT,\n                metadata TEXT,\n                FOREIGN KEY (raw_replay_id) REFERENCES raw_replays(replay_id),\n                CHECK (parse_status IN ('pending', 'parsed', 'failed'))\n            )\n        \"\"\")\n\n        # Indexes for fast lookups\n        cursor.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_raw_replays_downloaded\n            ON raw_replays(is_downloaded)\n        \"\"\")\n        cursor.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_raw_replays_status\n            ON raw_replays(download_status)\n        \"\"\")\n        cursor.execute(\"\"\"\n            CREATE INDEX IF NOT EXISTS idx_parsed_replays_status\n            ON parsed_replays(parse_status)\n        \"\"\")\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.add_replay","title":"add_replay","text":"<pre><code>add_replay(\n    replay_id: str, ballchasing_metadata: Dict\n) -&gt; bool\n</code></pre> <p>Add a raw replay to the database.</p> <p>Returns True if this is a NEW replay, False if it already existed.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def add_replay(self, replay_id: str, ballchasing_metadata: Dict) -&gt; bool:\n    \"\"\"\n    Add a raw replay to the database.\n\n    Returns True if this is a NEW replay, False if it already existed.\n    \"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n\n        cursor.execute(\"SELECT replay_id FROM raw_replays WHERE replay_id = ?\", (replay_id,))\n        if cursor.fetchone() is not None:\n            return False\n\n        blue = ballchasing_metadata.get('blue', {})\n        orange = ballchasing_metadata.get('orange', {})\n\n        cursor.execute(\"\"\"\n            INSERT INTO raw_replays (replay_id, title, date, blue_team, orange_team, is_downloaded)\n            VALUES (?, ?, ?, ?, ?, 0)\n        \"\"\", (\n            replay_id,\n            ballchasing_metadata.get('replay_title', 'Unknown'),\n            ballchasing_metadata.get('date'),\n            blue.get('name', 'Unknown'),\n            orange.get('name', 'Unknown')\n        ))\n\n        return True\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.register_group_download","title":"register_group_download","text":"<pre><code>register_group_download(\n    group_id: str, name: str, replay_count: int\n)\n</code></pre> <p>Track that we've downloaded this group from Ballchasing.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def register_group_download(self, group_id: str, name: str, replay_count: int):\n    \"\"\"Track that we've downloaded this group from Ballchasing.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            INSERT INTO groups (group_id, name, downloaded_at, replay_count)\n            VALUES (?, ?, ?, ?)\n            ON CONFLICT(group_id) DO UPDATE SET\n                downloaded_at = excluded.downloaded_at,\n                replay_count = excluded.replay_count\n        \"\"\", (group_id, name, datetime.now(timezone.utc).isoformat(), replay_count))\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.is_replay_downloaded","title":"is_replay_downloaded","text":"<pre><code>is_replay_downloaded(replay_id: str) -&gt; bool\n</code></pre> <p>Check if raw replay has been downloaded already.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def is_replay_downloaded(self, replay_id: str) -&gt; bool:\n    \"\"\"Check if raw replay has been downloaded already.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            SELECT is_downloaded FROM raw_replays\n            WHERE replay_id = ? AND is_downloaded = 1\n        \"\"\", (replay_id,))\n        return cursor.fetchone() is not None\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.mark_downloaded","title":"mark_downloaded","text":"<pre><code>mark_downloaded(\n    replay_id: str, storage_key: str, file_size: int\n)\n</code></pre> <p>Mark a raw replay as downloaded with its storage location.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def mark_downloaded(self, replay_id: str, storage_key: str, file_size: int):\n    \"\"\"Mark a raw replay as downloaded with its storage location.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            UPDATE raw_replays\n            SET storage_key = ?,\n                file_size_bytes = ?,\n                downloaded_at = ?,\n                is_downloaded = 1,\n                download_status = 'downloaded'\n            WHERE replay_id = ?\n        \"\"\", (storage_key, file_size, datetime.now(timezone.utc).isoformat(), replay_id))\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.mark_replay_failed","title":"mark_replay_failed","text":"<pre><code>mark_replay_failed(\n    replay_id: str, error_message: str = None\n)\n</code></pre> <p>Mark a raw replay download as failed.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def mark_replay_failed(self, replay_id: str, error_message: str = None):\n    \"\"\"Mark a raw replay download as failed.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            UPDATE raw_replays\n            SET download_status = 'failed',\n                error_message = ?\n            WHERE replay_id = ?\n        \"\"\", (error_message, replay_id))\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_failed_replays","title":"get_failed_replays","text":"<pre><code>get_failed_replays() -&gt; List[Dict]\n</code></pre> <p>Get all raw replays that failed to download.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def get_failed_replays(self) -&gt; List[Dict]:\n    \"\"\"Get all raw replays that failed to download.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            SELECT replay_id, title, error_message\n            FROM raw_replays\n            WHERE download_status = 'failed'\n        \"\"\")\n        return [dict(row) for row in cursor.fetchall()]\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_downloaded_replays","title":"get_downloaded_replays","text":"<pre><code>get_downloaded_replays(\n    limit: Optional[int] = None,\n) -&gt; List[Dict]\n</code></pre> <p>Get all successfully downloaded raw replays.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def get_downloaded_replays(self, limit: Optional[int] = None) -&gt; List[Dict]:\n    \"\"\"Get all successfully downloaded raw replays.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        query = \"\"\"\n            SELECT replay_id, title, date, blue_team, orange_team,\n                   storage_key, file_size_bytes, downloaded_at\n            FROM raw_replays\n            WHERE is_downloaded = 1\n        \"\"\"\n        if limit:\n            query += f\" LIMIT {limit}\"\n        cursor.execute(query)\n        return [dict(row) for row in cursor.fetchall()]\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.add_parsed_replay","title":"add_parsed_replay","text":"<pre><code>add_parsed_replay(\n    replay_id: str,\n    raw_replay_id: str,\n    output_path: str,\n    output_format: str,\n    fps: float,\n    frame_count: int,\n    feature_count: int,\n    file_size_bytes: int,\n    metadata: Optional[str] = None,\n) -&gt; bool\n</code></pre> <p>Register a successfully parsed replay.</p> <p>If the replay was previously marked as failed, updates it to parsed status.</p> <p>Parameters:</p> Name Type Description Default <code>replay_id</code> <code>str</code> <p>Unique ID for the parsed output (can be same as raw_replay_id)</p> required <code>raw_replay_id</code> <code>str</code> <p>ID of the source raw replay</p> required <code>output_path</code> <code>str</code> <p>Path to the parsed output file</p> required <code>output_format</code> <code>str</code> <p>Format of output (e.g., 'parquet', 'numpy')</p> required <code>fps</code> <code>float</code> <p>Frames per second used for parsing</p> required <code>frame_count</code> <code>int</code> <p>Number of frames in the parsed data</p> required <code>feature_count</code> <code>int</code> <p>Number of features in the parsed data</p> required <code>file_size_bytes</code> <code>int</code> <p>Size of the output file</p> required <code>metadata</code> <code>Optional[str]</code> <p>Optional JSON string with additional metadata</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if this is a new entry, False if updating existing entry</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def add_parsed_replay(\n    self,\n    replay_id: str,\n    raw_replay_id: str,\n    output_path: str,\n    output_format: str,\n    fps: float,\n    frame_count: int,\n    feature_count: int,\n    file_size_bytes: int,\n    metadata: Optional[str] = None\n) -&gt; bool:\n    \"\"\"\n    Register a successfully parsed replay.\n\n    If the replay was previously marked as failed, updates it to parsed status.\n\n    Args:\n        replay_id: Unique ID for the parsed output (can be same as raw_replay_id)\n        raw_replay_id: ID of the source raw replay\n        output_path: Path to the parsed output file\n        output_format: Format of output (e.g., 'parquet', 'numpy')\n        fps: Frames per second used for parsing\n        frame_count: Number of frames in the parsed data\n        feature_count: Number of features in the parsed data\n        file_size_bytes: Size of the output file\n        metadata: Optional JSON string with additional metadata\n\n    Returns:\n        True if this is a new entry, False if updating existing entry\n    \"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n\n        cursor.execute(\"\"\"\n            INSERT INTO parsed_replays (\n                replay_id, raw_replay_id, output_path, output_format,\n                fps, frame_count, feature_count, file_size_bytes,\n                parsed_at, parse_status, metadata\n            )\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, 'parsed', ?)\n            ON CONFLICT(replay_id) DO UPDATE SET\n                output_path = excluded.output_path,\n                output_format = excluded.output_format,\n                fps = excluded.fps,\n                frame_count = excluded.frame_count,\n                feature_count = excluded.feature_count,\n                file_size_bytes = excluded.file_size_bytes,\n                parsed_at = excluded.parsed_at,\n                parse_status = 'parsed',\n                error_message = NULL,\n                metadata = excluded.metadata\n        \"\"\", (\n            replay_id, raw_replay_id, output_path, output_format,\n            fps, frame_count, feature_count, file_size_bytes,\n            datetime.now(timezone.utc).isoformat(), metadata\n        ))\n\n        return cursor.rowcount &gt; 0\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.is_replay_parsed","title":"is_replay_parsed","text":"<pre><code>is_replay_parsed(replay_id: str) -&gt; bool\n</code></pre> <p>Check if a replay has been parsed already.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def is_replay_parsed(self, replay_id: str) -&gt; bool:\n    \"\"\"Check if a replay has been parsed already.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            SELECT replay_id FROM parsed_replays\n            WHERE replay_id = ? AND parse_status = 'parsed'\n        \"\"\", (replay_id,))\n        return cursor.fetchone() is not None\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.mark_parse_failed","title":"mark_parse_failed","text":"<pre><code>mark_parse_failed(\n    replay_id: str,\n    raw_replay_id: str,\n    error_message: str = None,\n)\n</code></pre> <p>Mark a replay parsing as failed.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def mark_parse_failed(self, replay_id: str, raw_replay_id: str, error_message: str = None):\n    \"\"\"Mark a replay parsing as failed.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            INSERT INTO parsed_replays (replay_id, raw_replay_id, parse_status, error_message)\n            VALUES (?, ?, 'failed', ?)\n            ON CONFLICT(replay_id) DO UPDATE SET\n                parse_status = 'failed',\n                error_message = excluded.error_message\n        \"\"\", (replay_id, raw_replay_id, error_message))\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_unparsed_replays","title":"get_unparsed_replays","text":"<pre><code>get_unparsed_replays(\n    limit: Optional[int] = None,\n) -&gt; List[Dict]\n</code></pre> <p>Get downloaded replays that haven't been successfully parsed yet.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def get_unparsed_replays(self, limit: Optional[int] = None) -&gt; List[Dict]:\n    \"\"\"Get downloaded replays that haven't been successfully parsed yet.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        query = \"\"\"\n            SELECT r.replay_id, r.title, r.storage_key, r.file_size_bytes\n            FROM raw_replays r\n            LEFT JOIN parsed_replays p ON r.replay_id = p.raw_replay_id\n                AND p.parse_status = 'parsed'\n            WHERE r.is_downloaded = 1 AND p.replay_id IS NULL\n        \"\"\"\n        if limit:\n            query += f\" LIMIT {limit}\"\n        cursor.execute(query)\n        return [dict(row) for row in cursor.fetchall()]\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_parsed_replays","title":"get_parsed_replays","text":"<pre><code>get_parsed_replays(\n    limit: Optional[int] = None,\n) -&gt; List[Dict]\n</code></pre> <p>Get all successfully parsed replays.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def get_parsed_replays(self, limit: Optional[int] = None) -&gt; List[Dict]:\n    \"\"\"Get all successfully parsed replays.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        query = \"\"\"\n            SELECT replay_id, raw_replay_id, output_path, output_format,\n                   fps, frame_count, feature_count, file_size_bytes, parsed_at\n            FROM parsed_replays\n            WHERE parse_status = 'parsed'\n        \"\"\"\n        if limit:\n            query += f\" LIMIT {limit}\"\n        cursor.execute(query)\n        return [dict(row) for row in cursor.fetchall()]\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_failed_parses","title":"get_failed_parses","text":"<pre><code>get_failed_parses() -&gt; List[Dict]\n</code></pre> <p>Get all replays that failed to parse.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def get_failed_parses(self) -&gt; List[Dict]:\n    \"\"\"Get all replays that failed to parse.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n        cursor.execute(\"\"\"\n            SELECT replay_id, raw_replay_id, error_message\n            FROM parsed_replays\n            WHERE parse_status = 'failed'\n        \"\"\")\n        return [dict(row) for row in cursor.fetchall()]\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_stats","title":"get_stats","text":"<pre><code>get_stats() -&gt; Dict\n</code></pre> <p>Get statistics for raw replays.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def get_stats(self) -&gt; Dict:\n    \"\"\"Get statistics for raw replays.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n\n        cursor.execute(\"SELECT COUNT(*) as total FROM raw_replays\")\n        total = cursor.fetchone()['total']\n\n        cursor.execute(\"SELECT COUNT(*) as downloaded FROM raw_replays WHERE is_downloaded = 1\")\n        downloaded = cursor.fetchone()['downloaded']\n\n        cursor.execute(\"SELECT SUM(file_size_bytes) as bytes FROM raw_replays WHERE is_downloaded = 1\")\n        total_bytes = cursor.fetchone()['bytes'] or 0\n\n        cursor.execute(\"SELECT COUNT(*) as failed FROM raw_replays WHERE download_status = 'failed'\")\n        failed = cursor.fetchone()['failed']\n\n        return {\n            'total_replays': total,\n            'downloaded': downloaded,\n            'failed': failed,\n            'pending': total - downloaded - failed,\n            'storage_mb': round(total_bytes / (1024**2), 2),\n            'storage_gb': round(total_bytes / (1024**3), 2)\n        }\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_parse_stats","title":"get_parse_stats","text":"<pre><code>get_parse_stats() -&gt; Dict\n</code></pre> <p>Get statistics for parsed replays.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def get_parse_stats(self) -&gt; Dict:\n    \"\"\"Get statistics for parsed replays.\"\"\"\n    with self.get_connection() as conn:\n        cursor = conn.cursor()\n\n        cursor.execute(\"SELECT COUNT(*) as total FROM parsed_replays\")\n        total = cursor.fetchone()['total']\n\n        cursor.execute(\"SELECT COUNT(*) as parsed FROM parsed_replays WHERE parse_status = 'parsed'\")\n        parsed = cursor.fetchone()['parsed']\n\n        cursor.execute(\"SELECT SUM(file_size_bytes) as bytes FROM parsed_replays WHERE parse_status = 'parsed'\")\n        total_bytes = cursor.fetchone()['bytes'] or 0\n\n        cursor.execute(\"SELECT COUNT(*) as failed FROM parsed_replays WHERE parse_status = 'failed'\")\n        failed = cursor.fetchone()['failed']\n\n        cursor.execute(\"SELECT SUM(frame_count) as frames FROM parsed_replays WHERE parse_status = 'parsed'\")\n        total_frames = cursor.fetchone()['frames'] or 0\n\n        return {\n            'total_entries': total,\n            'parsed': parsed,\n            'failed': failed,\n            'total_frames': total_frames,\n            'storage_mb': round(total_bytes / (1024**2), 2),\n            'storage_gb': round(total_bytes / (1024**3), 2)\n        }\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.database.ImpulseDB.get_full_stats","title":"get_full_stats","text":"<pre><code>get_full_stats() -&gt; Dict\n</code></pre> <p>Get combined statistics for raw and parsed replays.</p> Source code in <code>impulse/collection/database.py</code> <pre><code>def get_full_stats(self) -&gt; Dict:\n    \"\"\"Get combined statistics for raw and parsed replays.\"\"\"\n    return {\n        'raw': self.get_stats(),\n        'parsed': self.get_parse_stats()\n    }\n</code></pre>"},{"location":"collection/api-reference/#ratelimiter","title":"RateLimiter","text":""},{"location":"collection/api-reference/#impulse.collection.rate_limiter.RateLimiter","title":"RateLimiter","text":"<pre><code>RateLimiter(\n    requests_per_second: float = 1.0,\n    requests_per_hour: int = 200,\n)\n</code></pre> <p>Rate limiter for Ballchasing API.</p> <p>Limits: - 1 request per second - 200 requests per hour (free tier)</p> <p>Ballchasing patreon patrons get higher limits, but this is not implemented here.</p> <p>Initialize rate limiter.</p> <p>Parameters:</p> Name Type Description Default <code>requests_per_second</code> <code>float</code> <p>Maximum requests per second (default: 1)</p> <code>1.0</code> <code>requests_per_hour</code> <code>int</code> <p>Maximum requests per hour (default: 200)</p> <code>200</code> Source code in <code>impulse/collection/rate_limiter.py</code> <pre><code>def __init__(self, requests_per_second: float = 1.0, requests_per_hour: int = 200):\n    \"\"\"\n    Initialize rate limiter.\n\n    Args:\n        requests_per_second: Maximum requests per second (default: 1)\n        requests_per_hour: Maximum requests per hour (default: 200)\n    \"\"\"\n    self.requests_per_second = requests_per_second\n    self.requests_per_hour = requests_per_hour\n\n    self.last_request_time: Optional[datetime] = None\n    self.hourly_window_start: Optional[datetime] = None\n    self.requests_this_hour = 0\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.rate_limiter.RateLimiter.wait_if_needed","title":"wait_if_needed","text":"<pre><code>wait_if_needed()\n</code></pre> <p>Wait if necessary to comply with rate limits. Handles both per-second and per-hour limits.</p> Source code in <code>impulse/collection/rate_limiter.py</code> <pre><code>def wait_if_needed(self):\n    \"\"\"\n    Wait if necessary to comply with rate limits.\n    Handles both per-second and per-hour limits.\n    \"\"\"\n    now = datetime.now(timezone.utc)\n\n    # Initialize on first request\n    if self.hourly_window_start is None:\n        self.hourly_window_start = now\n        self.requests_this_hour = 0\n\n    # Check if we need to reset hourly counter\n    time_since_window_start = (now - self.hourly_window_start).total_seconds()\n    if time_since_window_start &gt;= 3600:\n        # Hour has passed, reset counter\n        self.hourly_window_start = now\n        self.requests_this_hour = 0\n        print(f\"\\n  \u23f1\ufe0f  Hourly rate limit window reset\")\n\n    # Check hourly limit\n    if self.requests_this_hour &gt;= self.requests_per_hour:\n        # Hit hourly limit, need to wait\n        time_until_reset = 3600 - time_since_window_start\n        print(f\"\\n  \u26a0\ufe0f  Hit hourly rate limit ({self.requests_per_hour} requests/hour)\")\n        print(f\"  \u23f8\ufe0f  Pausing for {time_until_reset/60:.1f} minutes until rate limit resets...\")\n        print(f\"  \u23f0  Will resume at: {(now + timedelta(seconds=time_until_reset)).strftime('%H:%M:%S UTC')}\")\n\n        # Sleep until window resets\n        time.sleep(time_until_reset + 1)  # +1 second buffer\n\n        # Reset counter\n        self.hourly_window_start = datetime.now(timezone.utc)\n        self.requests_this_hour = 0\n        print(f\"  \u2713 Resuming downloads...\")\n\n    # Check per-second limit\n    if self.last_request_time is not None:\n        time_since_last = (now - self.last_request_time).total_seconds()\n        min_interval = 1.0 / self.requests_per_second\n\n        if time_since_last &lt; min_interval:\n            sleep_time = min_interval - time_since_last\n            time.sleep(sleep_time)\n\n    # Update tracking\n    self.last_request_time = datetime.now(timezone.utc)\n    self.requests_this_hour += 1\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.rate_limiter.RateLimiter.get_status","title":"get_status","text":"<pre><code>get_status() -&gt; dict\n</code></pre> <p>Get current rate limiter status</p> Source code in <code>impulse/collection/rate_limiter.py</code> <pre><code>def get_status(self) -&gt; dict:\n    \"\"\"Get current rate limiter status\"\"\"\n    now = datetime.now(timezone.utc)\n\n    if self.hourly_window_start is None:\n        return {\n            'requests_this_hour': 0,\n            'requests_remaining': self.requests_per_hour,\n            'window_resets_in_seconds': None\n        }\n\n    time_since_window = (now - self.hourly_window_start).total_seconds()\n    time_until_reset = max(0, 3600 - time_since_window)\n\n    return {\n        'requests_this_hour': self.requests_this_hour,\n        'requests_remaining': self.requests_per_hour - self.requests_this_hour,\n        'window_resets_in_seconds': time_until_reset,\n        'window_resets_in_minutes': time_until_reset / 60\n    }\n</code></pre>"},{"location":"collection/api-reference/#utility-functions","title":"Utility Functions","text":""},{"location":"collection/api-reference/#impulse.collection.utils","title":"utils","text":"<p>Utility functions for the collection module.</p> <p>Contains helper functions for path sanitization, tree operations, and other common tasks used across the collection module.</p>"},{"location":"collection/api-reference/#impulse.collection.utils.sanitize_path_component","title":"sanitize_path_component","text":"<pre><code>sanitize_path_component(name: str) -&gt; str\n</code></pre> <p>Sanitize a string for safe use in file paths.</p> <p>Removes or replaces invalid characters that might cause issues in filesystem paths or S3 keys.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>String to sanitize (e.g., group name, replay title)</p> required <p>Returns:</p> Type Description <code>str</code> <p>Sanitized string safe for use in paths</p> Example <p>sanitize_path_component(\"RLCS 2024: Worlds\") 'RLCS 2024 Worlds' sanitize_path_component(\"Team/Name\") 'Team_Name_Bad_' Source code in <code>impulse/collection/utils.py</code> <pre><code>def sanitize_path_component(name: str) -&gt; str:\n    \"\"\"\n    Sanitize a string for safe use in file paths.\n\n    Removes or replaces invalid characters that might cause issues\n    in filesystem paths or S3 keys.\n\n    Args:\n        name: String to sanitize (e.g., group name, replay title)\n\n    Returns:\n        Sanitized string safe for use in paths\n\n    Example:\n        &gt;&gt;&gt; sanitize_path_component(\"RLCS 2024: Worlds\")\n        'RLCS 2024 Worlds'\n        &gt;&gt;&gt; sanitize_path_component(\"Team/Name&lt;Bad&gt;\")\n        'Team_Name_Bad_'\n    \"\"\"\n    # Characters that are invalid in file paths or problematic in S3\n    invalid_chars = '&lt;&gt;:\"/\\\\|?*'\n\n    for char in invalid_chars:\n        name = name.replace(char, '_')\n\n    # Remove leading/trailing dots and spaces\n    name = name.strip('. ')\n\n    return name\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.utils.flatten_group_tree","title":"flatten_group_tree","text":"<pre><code>flatten_group_tree(\n    tree: Dict, path: List[str] = None\n) -&gt; List[Tuple[Dict, List[str]]]\n</code></pre> <p>Flatten a hierarchical group tree into a list of (replay, path) tuples.</p> <p>Takes a nested group structure and returns a flat list where each replay is paired with its full hierarchical path.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>Dict</code> <p>Hierarchical tree structure with 'name', 'replays', and 'children'</p> required <code>path</code> <code>List[str]</code> <p>Current path (used for recursion, should be None on initial call)</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Tuple[Dict, List[str]]]</code> <p>List of (replay_dict, path_components) tuples</p> Example <p>tree = { ...     'name': 'RLCS 2024', ...     'replays': [], ...     'children': [{ ...         'name': 'Worlds', ...         'replays': [{'id': 'abc123', 'title': 'Grand Finals'}], ...         'children': [] ...     }] ... } flatten_group_tree(tree) [({'id': 'abc123', ...}, ['RLCS 2024', 'Worlds'])]</p> Source code in <code>impulse/collection/utils.py</code> <pre><code>def flatten_group_tree(tree: Dict, path: List[str] = None) -&gt; List[Tuple[Dict, List[str]]]:\n    \"\"\"\n    Flatten a hierarchical group tree into a list of (replay, path) tuples.\n\n    Takes a nested group structure and returns a flat list where each replay\n    is paired with its full hierarchical path.\n\n    Args:\n        tree: Hierarchical tree structure with 'name', 'replays', and 'children'\n        path: Current path (used for recursion, should be None on initial call)\n\n    Returns:\n        List of (replay_dict, path_components) tuples\n\n    Example:\n        &gt;&gt;&gt; tree = {\n        ...     'name': 'RLCS 2024',\n        ...     'replays': [],\n        ...     'children': [{\n        ...         'name': 'Worlds',\n        ...         'replays': [{'id': 'abc123', 'title': 'Grand Finals'}],\n        ...         'children': []\n        ...     }]\n        ... }\n        &gt;&gt;&gt; flatten_group_tree(tree)\n        [({'id': 'abc123', ...}, ['RLCS 2024', 'Worlds'])]\n    \"\"\"\n    if path is None:\n        path = []\n\n    result = []\n\n    # Add replays from this node with their full path\n    for replay in tree.get('replays', []):\n        result.append((replay, path + [tree['name']]))\n\n    # Recurse into children\n    for child in tree.get('children', []):\n        result.extend(flatten_group_tree(child, path + [tree['name']]))\n\n    return result\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.utils.build_path_components","title":"build_path_components","text":"<pre><code>build_path_components(\n    group_path: List[str],\n    root_name: str,\n    include_root: bool = True,\n) -&gt; List[str]\n</code></pre> <p>Build sanitized path components from a group hierarchy path.</p> <p>Parameters:</p> Name Type Description Default <code>group_path</code> <code>List[str]</code> <p>List of group names from the hierarchy</p> required <code>root_name</code> <code>str</code> <p>Name of the root group</p> required <code>include_root</code> <code>bool</code> <p>Whether to include root in the path</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of sanitized path components</p> Example <p>build_path_components(['RLCS 2024', 'Worlds', 'Day 1'], 'RLCS 2024') ['RLCS 2024', 'Worlds', 'Day 1'] build_path_components(['RLCS 2024', 'Worlds'], 'RLCS 2024', include_root=False) ['Worlds']</p> Source code in <code>impulse/collection/utils.py</code> <pre><code>def build_path_components(group_path: List[str], root_name: str,\n                         include_root: bool = True) -&gt; List[str]:\n    \"\"\"\n    Build sanitized path components from a group hierarchy path.\n\n    Args:\n        group_path: List of group names from the hierarchy\n        root_name: Name of the root group\n        include_root: Whether to include root in the path\n\n    Returns:\n        List of sanitized path components\n\n    Example:\n        &gt;&gt;&gt; build_path_components(['RLCS 2024', 'Worlds', 'Day 1'], 'RLCS 2024')\n        ['RLCS 2024', 'Worlds', 'Day 1']\n        &gt;&gt;&gt; build_path_components(['RLCS 2024', 'Worlds'], 'RLCS 2024', include_root=False)\n        ['Worlds']\n    \"\"\"\n    # Sanitize all components\n    sanitized = [sanitize_path_component(p) for p in group_path]\n\n    if not include_root and sanitized and sanitized[0] == sanitize_path_component(root_name):\n        # Remove root from path\n        sanitized = sanitized[1:]\n\n    return sanitized\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.utils.format_bytes","title":"format_bytes","text":"<pre><code>format_bytes(size_bytes: int) -&gt; str\n</code></pre> <p>Format byte size into human-readable string.</p> <p>Parameters:</p> Name Type Description Default <code>size_bytes</code> <code>int</code> <p>Size in bytes</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted string (e.g., \"1.5 GB\", \"234.2 MB\")</p> Example <p>format_bytes(1500000000) '1.40 GB' format_bytes(5000000) '4.77 MB'</p> Source code in <code>impulse/collection/utils.py</code> <pre><code>def format_bytes(size_bytes: int) -&gt; str:\n    \"\"\"\n    Format byte size into human-readable string.\n\n    Args:\n        size_bytes: Size in bytes\n\n    Returns:\n        Formatted string (e.g., \"1.5 GB\", \"234.2 MB\")\n\n    Example:\n        &gt;&gt;&gt; format_bytes(1500000000)\n        '1.40 GB'\n        &gt;&gt;&gt; format_bytes(5000000)\n        '4.77 MB'\n    \"\"\"\n    if size_bytes &lt; 1024:\n        return f\"{size_bytes} B\"\n    elif size_bytes &lt; 1024**2:\n        return f\"{size_bytes / 1024:.2f} KB\"\n    elif size_bytes &lt; 1024**3:\n        return f\"{size_bytes / (1024**2):.2f} MB\"\n    else:\n        return f\"{size_bytes / (1024**3):.2f} GB\"\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.utils.extract_replay_metadata","title":"extract_replay_metadata","text":"<pre><code>extract_replay_metadata(ballchasing_replay: Dict) -&gt; Dict\n</code></pre> <p>Extract relevant metadata from a Ballchasing replay response.</p> <p>Normalizes the Ballchasing API response format into a consistent metadata dictionary for storage.</p> <p>Parameters:</p> Name Type Description Default <code>ballchasing_replay</code> <code>Dict</code> <p>Replay dict from Ballchasing API</p> required <p>Returns:</p> Type Description <code>Dict</code> <p>Normalized metadata dict</p> Example <p>bc_replay = { ...     'id': 'abc123', ...     'title': 'Grand Finals', ...     'blue': {'name': 'Team A'}, ...     'orange': {'name': 'Team B'}, ...     'date': '2024-12-01T10:00:00' ... } extract_replay_metadata(bc_replay)</p> Source code in <code>impulse/collection/utils.py</code> <pre><code>def extract_replay_metadata(ballchasing_replay: Dict) -&gt; Dict:\n    \"\"\"\n    Extract relevant metadata from a Ballchasing replay response.\n\n    Normalizes the Ballchasing API response format into a consistent\n    metadata dictionary for storage.\n\n    Args:\n        ballchasing_replay: Replay dict from Ballchasing API\n\n    Returns:\n        Normalized metadata dict\n\n    Example:\n        &gt;&gt;&gt; bc_replay = {\n        ...     'id': 'abc123',\n        ...     'title': 'Grand Finals',\n        ...     'blue': {'name': 'Team A'},\n        ...     'orange': {'name': 'Team B'},\n        ...     'date': '2024-12-01T10:00:00'\n        ... }\n        &gt;&gt;&gt; extract_replay_metadata(bc_replay)\n        {'replay_id': 'abc123', 'title': 'Grand Finals', ...}\n    \"\"\"\n    blue = ballchasing_replay.get('blue', {})\n    orange = ballchasing_replay.get('orange', {})\n\n    return {\n        'replay_id': ballchasing_replay.get('id'),\n        'title': ballchasing_replay.get('replay_title', ballchasing_replay.get('title', 'Unknown')),\n        'blue_team': blue.get('name', 'Unknown'),\n        'orange_team': orange.get('name', 'Unknown'),\n        'date': ballchasing_replay.get('date', 'Unknown'),\n        'source': 'ballchasing'\n    }\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.utils.get_tree_cache_path","title":"get_tree_cache_path","text":"<pre><code>get_tree_cache_path(\n    group_id: str, cache_dir: Optional[Path] = None\n) -&gt; Path\n</code></pre> <p>Get the cache file path for a group tree.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Ballchasing group ID</p> required <code>cache_dir</code> <code>Optional[Path]</code> <p>Optional directory for cache files (defaults to ./replays/raw/cache)</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the cache file</p> Source code in <code>impulse/collection/utils.py</code> <pre><code>def get_tree_cache_path(group_id: str, cache_dir: Optional[Path] = None) -&gt; Path:\n    \"\"\"\n    Get the cache file path for a group tree.\n\n    Args:\n        group_id: Ballchasing group ID\n        cache_dir: Optional directory for cache files (defaults to ./replays/raw/cache)\n\n    Returns:\n        Path to the cache file\n    \"\"\"\n    if cache_dir is None:\n        cache_dir = Path(\"./replays/raw/cache\")\n\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    return cache_dir / f\"group_tree_{group_id}.json\"\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.utils.save_group_tree","title":"save_group_tree","text":"<pre><code>save_group_tree(\n    tree: Dict,\n    group_id: str,\n    cache_dir: Optional[Path] = None,\n) -&gt; Path\n</code></pre> <p>Save a group tree to a JSON cache file.</p> <p>Parameters:</p> Name Type Description Default <code>tree</code> <code>Dict</code> <p>Group tree structure from build_group_tree()</p> required <code>group_id</code> <code>str</code> <p>Ballchasing group ID</p> required <code>cache_dir</code> <code>Optional[Path]</code> <p>Optional directory for cache files</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved cache file</p> Example <p>tree = client.build_group_tree('rlcs-2024-abc123') cache_path = save_group_tree(tree, 'rlcs-2024-abc123') print(f\"Tree cached at: {cache_path}\")</p> Source code in <code>impulse/collection/utils.py</code> <pre><code>def save_group_tree(tree: Dict, group_id: str, cache_dir: Optional[Path] = None) -&gt; Path:\n    \"\"\"\n    Save a group tree to a JSON cache file.\n\n    Args:\n        tree: Group tree structure from build_group_tree()\n        group_id: Ballchasing group ID\n        cache_dir: Optional directory for cache files\n\n    Returns:\n        Path to the saved cache file\n\n    Example:\n        &gt;&gt;&gt; tree = client.build_group_tree('rlcs-2024-abc123')\n        &gt;&gt;&gt; cache_path = save_group_tree(tree, 'rlcs-2024-abc123')\n        &gt;&gt;&gt; print(f\"Tree cached at: {cache_path}\")\n    \"\"\"\n    cache_path = get_tree_cache_path(group_id, cache_dir)\n\n    with open(cache_path, 'w') as f:\n        json.dump(tree, f, indent=2)\n\n    return cache_path\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.utils.load_group_tree","title":"load_group_tree","text":"<pre><code>load_group_tree(\n    group_id: str, cache_dir: Optional[Path] = None\n) -&gt; Optional[Dict]\n</code></pre> <p>Load a group tree from a JSON cache file.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Ballchasing group ID</p> required <code>cache_dir</code> <code>Optional[Path]</code> <p>Optional directory for cache files</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[Dict]</code> <p>Cached tree structure, or None if cache doesn't exist</p> Example <p>tree = load_group_tree('rlcs-2024-abc123') if tree: ...     replay_list = flatten_group_tree(tree)</p> Source code in <code>impulse/collection/utils.py</code> <pre><code>def load_group_tree(group_id: str, cache_dir: Optional[Path] = None) -&gt; Optional[Dict]:\n    \"\"\"\n    Load a group tree from a JSON cache file.\n\n    Args:\n        group_id: Ballchasing group ID\n        cache_dir: Optional directory for cache files\n\n    Returns:\n        Cached tree structure, or None if cache doesn't exist\n\n    Example:\n        &gt;&gt;&gt; tree = load_group_tree('rlcs-2024-abc123')\n        &gt;&gt;&gt; if tree:\n        ...     replay_list = flatten_group_tree(tree)\n    \"\"\"\n    cache_path = get_tree_cache_path(group_id, cache_dir)\n\n    if not cache_path.exists():\n        return None\n\n    with open(cache_path, 'r') as f:\n        return json.load(f)\n</code></pre>"},{"location":"collection/api-reference/#impulse.collection.utils.delete_group_tree_cache","title":"delete_group_tree_cache","text":"<pre><code>delete_group_tree_cache(\n    group_id: str, cache_dir: Optional[Path] = None\n) -&gt; bool\n</code></pre> <p>Delete a cached group tree file.</p> <p>Parameters:</p> Name Type Description Default <code>group_id</code> <code>str</code> <p>Ballchasing group ID</p> required <code>cache_dir</code> <code>Optional[Path]</code> <p>Optional directory for cache files</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if file was deleted, False if it didn't exist</p> Source code in <code>impulse/collection/utils.py</code> <pre><code>def delete_group_tree_cache(group_id: str, cache_dir: Optional[Path] = None) -&gt; bool:\n    \"\"\"\n    Delete a cached group tree file.\n\n    Args:\n        group_id: Ballchasing group ID\n        cache_dir: Optional directory for cache files\n\n    Returns:\n        True if file was deleted, False if it didn't exist\n    \"\"\"\n    cache_path = get_tree_cache_path(group_id, cache_dir)\n\n    if cache_path.exists():\n        cache_path.unlink()\n        return True\n\n    return False\n</code></pre>"},{"location":"collection/downloading/","title":"Downloading Replays","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"},{"location":"collection/overview/","title":"Collection Overview","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"},{"location":"collection/rlcs/","title":"RLCS Seasons","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"},{"location":"collection/storage/","title":"Storage Backends","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"},{"location":"configuration/#collectionconfig","title":"CollectionConfig","text":""},{"location":"configuration/#impulse.config.collection_config.CollectionConfig","title":"CollectionConfig  <code>dataclass</code>","text":"<pre><code>CollectionConfig(\n    ballchasing_api_key: str,\n    aws_region: Optional[str] = None,\n    s3_bucket_name: Optional[str] = None,\n    database_path: str = \"./impulse.db\",\n    rate_limit_per_second: float = 1.0,\n    rate_limit_per_hour: int = 200,\n)\n</code></pre> <p>Configuration for the Impulse collection module.</p> <p>Attributes:</p> Name Type Description <code>ballchasing_api_key</code> <code>str</code> <p>API key for Ballchasing.com</p> <code>aws_region</code> <code>Optional[str]</code> <p>AWS region for S3 storage</p> <code>s3_bucket_name</code> <code>Optional[str]</code> <p>S3 bucket name for replay storage</p> <code>database_path</code> <code>str</code> <p>Path to SQLite database file</p> <code>rate_limit_per_second</code> <code>float</code> <p>Max requests per second to Ballchasing API</p> <code>rate_limit_per_hour</code> <code>int</code> <p>Max requests per hour to Ballchasing API</p>"},{"location":"configuration/#impulse.config.collection_config.CollectionConfig.from_env","title":"from_env  <code>classmethod</code>","text":"<pre><code>from_env() -&gt; CollectionConfig\n</code></pre> <p>Load configuration from environment variables (.env file).</p> Required environment variables <ul> <li>BALLCHASING_API_KEY: Your Ballchasing API key</li> </ul> Optional environment variables <ul> <li>AWS_REGION: AWS region (required for S3 storage)</li> <li>S3_BUCKET_NAME: S3 bucket name (required for S3 storage)</li> </ul> <p>Returns:</p> Type Description <code>CollectionConfig</code> <p>CollectionConfig instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required environment variables are missing</p> Source code in <code>impulse/config/collection_config.py</code> <pre><code>@classmethod\ndef from_env(cls) -&gt; 'CollectionConfig':\n    \"\"\"\n    Load configuration from environment variables (.env file).\n\n    Required environment variables:\n        - BALLCHASING_API_KEY: Your Ballchasing API key\n\n    Optional environment variables:\n        - AWS_REGION: AWS region (required for S3 storage)\n        - S3_BUCKET_NAME: S3 bucket name (required for S3 storage)\n\n    Returns:\n        CollectionConfig instance\n\n    Raises:\n        ValueError: If required environment variables are missing\n    \"\"\"\n    load_dotenv()\n\n    ballchasing_api_key = os.environ.get(\"BALLCHASING_API_KEY\")\n    if not ballchasing_api_key:\n        raise ValueError(\n            \"BALLCHASING_API_KEY not found in environment variables. \"\n            \"Please add it to your .env file in the project root.\"\n        )\n\n    # AWS credentials are optional (only needed for S3 backend)\n    aws_region = os.environ.get(\"AWS_REGION\")\n    s3_bucket_name = os.environ.get(\"S3_BUCKET_NAME\")\n\n    return cls(\n        ballchasing_api_key=ballchasing_api_key,\n        aws_region=aws_region,\n        s3_bucket_name=s3_bucket_name\n    )\n</code></pre>"},{"location":"configuration/#impulse.config.collection_config.CollectionConfig.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(config_dict: dict) -&gt; CollectionConfig\n</code></pre> <p>Create configuration from a dictionary.</p> <p>Useful for testing or programmatic configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>dict</code> <p>Dictionary with configuration values</p> required <p>Returns:</p> Type Description <code>CollectionConfig</code> <p>CollectionConfig instance</p> Example <p>config = CollectionConfig.from_dict({ ...     'ballchasing_api_key': 'test_key', ...     'database_path': './test.db' ... })</p> Source code in <code>impulse/config/collection_config.py</code> <pre><code>@classmethod\ndef from_dict(cls, config_dict: dict) -&gt; 'CollectionConfig':\n    \"\"\"\n    Create configuration from a dictionary.\n\n    Useful for testing or programmatic configuration.\n\n    Args:\n        config_dict: Dictionary with configuration values\n\n    Returns:\n        CollectionConfig instance\n\n    Example:\n        &gt;&gt;&gt; config = CollectionConfig.from_dict({\n        ...     'ballchasing_api_key': 'test_key',\n        ...     'database_path': './test.db'\n        ... })\n    \"\"\"\n    return cls(**config_dict)\n</code></pre>"},{"location":"configuration/#impulse.config.collection_config.CollectionConfig.validate_for_s3","title":"validate_for_s3","text":"<pre><code>validate_for_s3() -&gt; None\n</code></pre> <p>Validate that required S3 configuration is present.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If AWS_REGION or S3_BUCKET_NAME is missing</p> Source code in <code>impulse/config/collection_config.py</code> <pre><code>def validate_for_s3(self) -&gt; None:\n    \"\"\"\n    Validate that required S3 configuration is present.\n\n    Raises:\n        ValueError: If AWS_REGION or S3_BUCKET_NAME is missing\n    \"\"\"\n    if not self.aws_region:\n        raise ValueError(\n            \"AWS_REGION not configured. Required for S3 storage backend.\"\n        )\n    if not self.s3_bucket_name:\n        raise ValueError(\n            \"S3_BUCKET_NAME not configured. Required for S3 storage backend.\"\n        )\n</code></pre>"},{"location":"configuration/#impulse.config.collection_config.CollectionConfig.__repr__","title":"__repr__","text":"<pre><code>__repr__() -&gt; str\n</code></pre> <p>String representation with masked API key.</p> Source code in <code>impulse/config/collection_config.py</code> <pre><code>def __repr__(self) -&gt; str:\n    \"\"\"String representation with masked API key.\"\"\"\n    return (\n        f\"CollectionConfig(\\n\"\n        f\"  ballchasing_api_key=***{self.ballchasing_api_key[-4:] if self.ballchasing_api_key else 'None'},\\n\"\n        f\"  aws_region={self.aws_region},\\n\"\n        f\"  s3_bucket_name={self.s3_bucket_name},\\n\"\n        f\"  database_path={self.database_path},\\n\"\n        f\"  rate_limits={self.rate_limit_per_second}/sec, {self.rate_limit_per_hour}/hour\\n\"\n        f\")\"\n    )\n</code></pre>"},{"location":"configuration/#parsingconfig","title":"ParsingConfig","text":""},{"location":"configuration/#impulse.config.parsing_config.ParsingConfig","title":"ParsingConfig","text":"<p>Configuration and utilities for replay parsing using subtr_actor.</p>"},{"location":"configuration/#impulse.config.parsing_config.ParsingConfig.get_preset","title":"get_preset  <code>classmethod</code>","text":"<pre><code>get_preset(preset_name: str) -&gt; Dict[str, List[str]]\n</code></pre> <p>Get feature preset by name.</p> <p>Parameters:</p> Name Type Description Default <code>preset_name</code> <code>str</code> <p>Name of preset ('minimal', 'standard', 'comprehensive')</p> required <p>Returns:</p> Type Description <code>Dict[str, List[str]]</code> <p>Dict with 'global' and 'player' feature lists</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If preset name not found</p> Source code in <code>impulse/config/parsing_config.py</code> <pre><code>@classmethod\ndef get_preset(cls, preset_name: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Get feature preset by name.\n\n    Args:\n        preset_name: Name of preset ('minimal', 'standard', 'comprehensive')\n\n    Returns:\n        Dict with 'global' and 'player' feature lists\n\n    Raises:\n        ValueError: If preset name not found\n    \"\"\"\n    if preset_name not in FEATURE_PRESETS:\n        raise ValueError(\n            f\"Unknown preset '{preset_name}'. \"\n            f\"Available: {list(FEATURE_PRESETS.keys())}\"\n        )\n    return FEATURE_PRESETS[preset_name]\n</code></pre>"},{"location":"configuration/#impulse.config.parsing_config.ParsingConfig.validate_features","title":"validate_features  <code>classmethod</code>","text":"<pre><code>validate_features(\n    global_features: List[str], player_features: List[str]\n) -&gt; None\n</code></pre> <p>Validate feature names before parsing.</p> <p>Parameters:</p> Name Type Description Default <code>global_features</code> <code>List[str]</code> <p>List of global feature names</p> required <code>player_features</code> <code>List[str]</code> <p>List of player feature names</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If any feature name is invalid</p> Source code in <code>impulse/config/parsing_config.py</code> <pre><code>@classmethod\ndef validate_features(cls, global_features: List[str], player_features: List[str]) -&gt; None:\n    \"\"\"Validate feature names before parsing.\n\n    Args:\n        global_features: List of global feature names\n        player_features: List of player feature names\n\n    Raises:\n        ValueError: If any feature name is invalid\n    \"\"\"\n    valid_global = set(VALID_FEATURE_ADDERS['global'].keys())\n    valid_player = set(VALID_FEATURE_ADDERS['player'].keys())\n\n    for feature in global_features:\n        if feature not in valid_global:\n            raise ValueError(\n                f\"Invalid global feature '{feature}'. \"\n                f\"Valid options: {sorted(valid_global)}\"\n            )\n\n    for feature in player_features:\n        if feature not in valid_player:\n            raise ValueError(\n                f\"Invalid player feature '{feature}'. \"\n                f\"Valid options: {sorted(valid_player)}\"\n            )\n</code></pre>"},{"location":"configuration/#impulse.config.parsing_config.ParsingConfig.get_column_names","title":"get_column_names  <code>classmethod</code>","text":"<pre><code>get_column_names(\n    feature_adder_name: str, feature_adder_type: str\n) -&gt; List[str]\n</code></pre> <p>Get returned column names for a feature adder.</p> <p>Parameters:</p> Name Type Description Default <code>feature_adder_name</code> <code>str</code> <p>Name of the feature adder</p> required <code>feature_adder_type</code> <code>str</code> <p>'global' or 'player'</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List of column names returned by subtr_actor.get_ndarray_with_info_from_replay_filepath() for that feature adder</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If feature adder not found</p> Source code in <code>impulse/config/parsing_config.py</code> <pre><code>@classmethod\ndef get_column_names(cls, feature_adder_name: str, feature_adder_type: str) -&gt; List[str]:\n    \"\"\"Get returned column names for a feature adder.\n\n    Args:\n        feature_adder_name: Name of the feature adder\n        feature_adder_type: 'global' or 'player'\n\n    Returns:\n        List of column names returned by subtr_actor.get_ndarray_with_info_from_replay_filepath() for that feature adder\n\n    Raises:\n        ValueError: If feature adder not found\n    \"\"\"\n    if feature_adder_type not in ['global', 'player']:\n        raise ValueError(f\"feature_adder_type must be 'global' or 'player', got '{feature_adder_type}'\")\n\n    valid_adders = VALID_FEATURE_ADDERS[feature_adder_type]\n    if feature_adder_name not in valid_adders:\n        raise ValueError(\n            f\"Feature adder '{feature_adder_name}' not found in {feature_adder_type} features. \"\n            f\"Valid options: {sorted(valid_adders.keys())}\"\n        )\n\n    return valid_adders[feature_adder_name]\n</code></pre>"},{"location":"configuration/#pipelineconfig","title":"PipelineConfig","text":""},{"location":"configuration/#impulse.config.pipeline_config.PipelineConfig","title":"PipelineConfig  <code>dataclass</code>","text":"<pre><code>PipelineConfig(\n    SCHEMA_MAX_PLAYERS: int = 8,\n    PARQUET_COMPRESSION: str = \"snappy\",\n    S3_PARSED_PREFIX: str = \"replays-parsed\",\n    S3_RAW_PREFIX: str = \"replays-raw\",\n    MIN_FRAMES: int = 100,\n    MAX_FRAMES: int = 100000,\n    MIN_PLAYERS: int = 2,\n    MAX_PLAYERS: int = 8,\n    DEDUPLICATE_POSITION: bool = True,\n    KEEP_QUATERNIONS: bool = True,\n    KEEP_EULER_ANGLES: bool = False,\n    KEEP_VELOCITIES: bool = True,\n    MAX_RETRIES: int = 3,\n    RETRY_DELAY_SECONDS: int = 60,\n)\n</code></pre> <p>Configuration for the replay processing pipeline.</p> <p>Can be instantiated with custom values or use defaults.</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.12+</li> <li>Ballchasing API key (optional \u2014 only needed for downloading replays)</li> <li>AWS credentials (optional \u2014 only needed for S3 storage)</li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":"<pre><code>git clone https://github.com/darmendariz/impulse.git\ncd impulse\nuv sync\nuv pip install -e .\n</code></pre> <p>After running <code>uv sync</code>, activate the virtual environment:</p> <pre><code>source .venv/bin/activate\n</code></pre> <p>Or run scripts directly without activating:</p> <pre><code>.venv/bin/python scripts/your_script.py\n</code></pre>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":"<p>Create a <code>.env</code> file in the project root:</p> <pre><code>BALLCHASING_API_KEY=your_api_key_here\n\n# Optional \u2014 required only for S3 storage\nAWS_REGION=your_aws_region\nS3_BUCKET_NAME=your-bucket-name\n</code></pre>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":""},{"location":"getting-started/quickstart/#download-replays-to-local-storage","title":"Download replays to local storage","text":"<pre><code>from impulse.collection import download_group\n\nresult = download_group(\n    group_id='insert-ballchasing-group-id',\n    storage_type='local',\n    output_dir='./replays'\n)\n\nprint(f\"Downloaded {result.successful} replays\")\n</code></pre>"},{"location":"getting-started/quickstart/#parse-a-replay-file","title":"Parse a replay file","text":"<pre><code>from impulse.parsing import ReplayParser\n\nparser = ReplayParser.from_preset('standard', fps=10.0)\nresult = parser.parse_file('./replays/my_replay.replay')\n\nif result.success:\n    array = result.array  # NumPy array, shape: (frames, features)\n    metadata = result.metadata\n\n    print(f\"Shape: {array.shape}\")\n    print(f\"Duration: {result.duration_seconds:.1f}s\")\n</code></pre>"},{"location":"getting-started/quickstart/#custom-feature-extraction-with-dataframe-output","title":"Custom feature extraction with DataFrame output","text":"<pre><code>from impulse.parsing import ReplayParser\nfrom impulse.parsing.parse_result_formatter import ParseResultFormatter\n\nparser = ReplayParser(\n    global_features=['BallRigidBody', 'SecondsRemaining'],\n    player_features=['PlayerRigidBody', 'PlayerBoost', 'PlayerAnyJump'],\n    fps=30.0\n)\n\nresult = parser.parse_file('./replays/my_replay.replay')\n\nif result.success:\n    formatter = ParseResultFormatter()\n    formatted = formatter.format(result)\n\n    df = formatted.dataframe\n    print(f\"DataFrame shape: {df.shape}\")\n\n    # Save to Parquet\n    formatter.save_to_parquet(formatted, './output')\n</code></pre>"},{"location":"getting-started/quickstart/#download-a-full-rlcs-season","title":"Download a full RLCS season","text":"<pre><code>from impulse.collection import RLCSManager\n\nrlcs = RLCSManager(storage_type='local', output_dir='./replays')\nrlcs.download_season('2024')\n</code></pre>"},{"location":"getting-started/quickstart/#whats-next","title":"What's next?","text":"<ul> <li>Collection overview \u2014 understand how downloading and storage works</li> <li>Parsing overview \u2014 learn about feature extraction and output formats</li> <li>Configuration \u2014 tune behavior for your use case</li> </ul>"},{"location":"parsing/api-reference/","title":"Parsing API Reference","text":""},{"location":"parsing/api-reference/#replayparser","title":"ReplayParser","text":""},{"location":"parsing/api-reference/#impulse.parsing.replay_parser.ReplayParser","title":"ReplayParser","text":"<pre><code>ReplayParser(\n    global_features: List[str],\n    player_features: List[str],\n    fps: float = ParsingConfig.DEFAULT_FPS,\n)\n</code></pre> <p>Parser for Rocket League replay files using subtr-actor.</p> <p>This class wraps the subtr-actor library and provides a clean interface for parsing replay files with specified features at a given frame sampling rate.</p> <p>Initialize the replay parser.</p> <p>Parameters:</p> Name Type Description Default <code>global_features</code> <code>List[str]</code> <p>List of global feature adders (e.g., 'BallRigidBody')</p> required <code>player_features</code> <code>List[str]</code> <p>List of player feature adders (e.g., 'PlayerBoost')</p> required <code>fps</code> <code>float</code> <p>Frames per second to sample at</p> <code>DEFAULT_FPS</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If feature adders are invalid or FPS is out of range</p> Source code in <code>impulse/parsing/replay_parser.py</code> <pre><code>def __init__(self, global_features: List[str], player_features: List[str], fps: float = ParsingConfig.DEFAULT_FPS):\n    \"\"\"\n    Initialize the replay parser.\n\n    Args:\n        global_features: List of global feature adders (e.g., 'BallRigidBody')\n        player_features: List of player feature adders (e.g., 'PlayerBoost')\n        fps: Frames per second to sample at\n\n    Raises:\n        ValueError: If feature adders are invalid or FPS is out of range\n    \"\"\"\n    # Validate features\n    ParsingConfig.validate_features(global_features, player_features)\n\n    # Validate FPS\n    if not ParsingConfig.MIN_FPS &lt;= fps &lt;= ParsingConfig.MAX_FPS:\n        raise ValueError(\n            f\"FPS must be between {ParsingConfig.MIN_FPS} and {ParsingConfig.MAX_FPS}, \"\n            f\"got {fps}\"\n        )\n\n    self.global_features = global_features\n    self.player_features = player_features\n    self.fps = fps\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.replay_parser.ReplayParser.from_preset","title":"from_preset  <code>classmethod</code>","text":"<pre><code>from_preset(\n    preset_name: str, fps: float = ParsingConfig.DEFAULT_FPS\n) -&gt; ReplayParser\n</code></pre> <p>Create parser from a feature preset.</p> <p>Parameters:</p> Name Type Description Default <code>preset_name</code> <code>str</code> <p>Name of preset ('minimal', 'standard', 'comprehensive')</p> required <code>fps</code> <code>float</code> <p>Frames per second to sample at</p> <code>DEFAULT_FPS</code> <p>Returns:</p> Type Description <code>ReplayParser</code> <p>ReplayParser instance</p> Example <p>parser = ReplayParser.from_preset('standard', fps=30.0)</p> Source code in <code>impulse/parsing/replay_parser.py</code> <pre><code>@classmethod\ndef from_preset(cls, preset_name: str, fps: float = ParsingConfig.DEFAULT_FPS) -&gt; \"ReplayParser\":\n    \"\"\"\n    Create parser from a feature preset.\n\n    Args:\n        preset_name: Name of preset ('minimal', 'standard', 'comprehensive')\n        fps: Frames per second to sample at\n\n    Returns:\n        ReplayParser instance\n\n    Example:\n        parser = ReplayParser.from_preset('standard', fps=30.0)\n    \"\"\"\n    preset = ParsingConfig.get_preset(preset_name)\n    return cls(\n        global_features=preset['global'],\n        player_features=preset['player'],\n        fps=fps\n    )\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.replay_parser.ReplayParser.parse_file","title":"parse_file","text":"<pre><code>parse_file(replay_path: str) -&gt; ParseResult\n</code></pre> <p>Parse a replay file from disk.</p> <p>Parameters:</p> Name Type Description Default <code>replay_path</code> <code>str</code> <p>Path to .replay file</p> required <p>Returns:</p> Type Description <code>ParseResult</code> <p>ParseResult with parsed data or error information</p> Source code in <code>impulse/parsing/replay_parser.py</code> <pre><code>def parse_file(self, replay_path: str) -&gt; ParseResult:\n    \"\"\"\n    Parse a replay file from disk.\n\n    Args:\n        replay_path: Path to .replay file\n\n    Returns:\n        ParseResult with parsed data or error information\n    \"\"\"\n    replay_path = str(Path(replay_path).resolve())\n\n    # Check file exists\n    if not Path(replay_path).exists():\n        return ParseResult(\n            success=False,\n            replay_path=replay_path,\n            metadata=None,\n            array=None,\n            num_frames=-1,\n            num_features=-1,\n            num_players=-1,\n            fps=self.fps,\n            global_features=self.global_features,\n            player_features=self.player_features,\n            error=f\"File not found: {replay_path}\"\n        )\n\n    try:\n        # Call subtr-actor\n        metadata, array = subtr_actor.get_ndarray_with_info_from_replay_filepath(\n            replay_path,\n            self.global_features,\n            self.player_features,\n            self.fps\n        )\n\n        # Extract player count from metadata\n        num_players = self._count_players(metadata)\n\n        # Validate result\n        validation_error = self._validate_parse_result(array)\n        if validation_error:\n            return ParseResult(\n                success=False,\n                replay_path=replay_path,\n                metadata=metadata,\n                array=array,\n                num_frames=array.shape[0] if array is not None else 0,\n                num_features=array.shape[1] if array is not None else 0,\n                num_players=num_players,\n                fps=self.fps,\n                global_features=self.global_features,\n                player_features=self.player_features,\n                error=validation_error\n            )\n\n        return ParseResult(\n            success=True,\n            replay_path=replay_path,\n            metadata=metadata,\n            array=array,\n            num_frames=array.shape[0],\n            num_features=array.shape[1],\n            num_players=num_players,\n            fps=self.fps,\n            global_features=self.global_features,\n            player_features=self.player_features\n        )\n\n    except Exception as e:\n        return ParseResult(\n            success=False,\n            replay_path=replay_path,\n            metadata=None,\n            array=None,\n            num_frames=-1,\n            num_features=-1,\n            num_players=-1,\n            fps=self.fps,\n            global_features=self.global_features,\n            player_features=self.player_features,\n            error=f\"Parsing failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"parsing/api-reference/#parsingpipeline","title":"ParsingPipeline","text":""},{"location":"parsing/api-reference/#impulse.parsing.parsing_pipeline.ParsingPipeline","title":"ParsingPipeline","text":"<pre><code>ParsingPipeline(\n    parser: ReplayParser,\n    db: Optional[ImpulseDB] = None,\n    formatter: Optional[ParseResultFormatter] = None,\n    progress_callback: Optional[\n        Callable[[PipelineProgress], None]\n    ] = None,\n)\n</code></pre> <p>High-level replay parsing orchestrator.</p> <p>Coordinates parsing from raw replays to formatted output, with database tracking and progress reporting.</p> Example <p>from impulse.parsing import ParsingPipeline, ReplayParser from impulse.collection.database import ImpulseDB</p> <p>parser = ReplayParser.from_preset('standard', fps=10.0) db = ImpulseDB() pipeline = ParsingPipeline(parser, db)</p> <p>Initialize the parsing pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>parser</code> <code>ReplayParser</code> <p>ReplayParser instance configured with features and FPS</p> required <code>db</code> <code>Optional[ImpulseDB]</code> <p>Optional database for tracking (enables deduplication and registration)</p> <code>None</code> <code>formatter</code> <code>Optional[ParseResultFormatter]</code> <p>Optional ParseResultFormatter (uses default if not provided)</p> <code>None</code> <code>progress_callback</code> <code>Optional[Callable[[PipelineProgress], None]]</code> <p>Optional callback for progress updates</p> <code>None</code> Source code in <code>impulse/parsing/parsing_pipeline.py</code> <pre><code>def __init__(\n    self,\n    parser: ReplayParser,\n    db: Optional[ImpulseDB] = None,\n    formatter: Optional[ParseResultFormatter] = None,\n    progress_callback: Optional[Callable[[PipelineProgress], None]] = None\n):\n    \"\"\"\n    Initialize the parsing pipeline.\n\n    Args:\n        parser: ReplayParser instance configured with features and FPS\n        db: Optional database for tracking (enables deduplication and registration)\n        formatter: Optional ParseResultFormatter (uses default if not provided)\n        progress_callback: Optional callback for progress updates\n    \"\"\"\n    self.parser = parser\n    self.db = db\n    self.formatter = formatter or ParseResultFormatter()\n    self.progress_callback = progress_callback or self._default_progress_callback\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.parsing_pipeline.ParsingPipeline--parse-a-single-replay","title":"Parse a single replay","text":"<p>result = pipeline.parse_replay('./replay.replay', './output')</p>"},{"location":"parsing/api-reference/#impulse.parsing.parsing_pipeline.ParsingPipeline--parse-multiple-replays","title":"Parse multiple replays","text":"<p>result = pipeline.parse_replays(replay_paths, './output')</p>"},{"location":"parsing/api-reference/#impulse.parsing.parsing_pipeline.ParsingPipeline--parse-all-unparsed-replays-from-database","title":"Parse all unparsed replays from database","text":"<p>result = pipeline.parse_unparsed('./raw_replays', './output')</p>"},{"location":"parsing/api-reference/#impulse.parsing.parsing_pipeline.ParsingPipeline.parse_replay","title":"parse_replay","text":"<pre><code>parse_replay(\n    replay_path: str,\n    output_dir: str,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION,\n) -&gt; FormatResult\n</code></pre> <p>Parse a single replay file and save to output directory.</p> <p>Parameters:</p> Name Type Description Default <code>replay_path</code> <code>str</code> <p>Path to .replay file</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output files</p> required <code>compression</code> <code>str</code> <p>Parquet compression algorithm</p> <code>PARQUET_COMPRESSION</code> <p>Returns:</p> Type Description <code>FormatResult</code> <p>FormatResult with parsing outcome</p> Source code in <code>impulse/parsing/parsing_pipeline.py</code> <pre><code>def parse_replay(\n    self,\n    replay_path: str,\n    output_dir: str,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION\n) -&gt; FormatResult:\n    \"\"\"\n    Parse a single replay file and save to output directory.\n\n    Args:\n        replay_path: Path to .replay file\n        output_dir: Directory to save output files\n        compression: Parquet compression algorithm\n\n    Returns:\n        FormatResult with parsing outcome\n    \"\"\"\n    replay_path = Path(replay_path)\n    replay_id = replay_path.stem\n\n    # Check if already parsed\n    if self.db and self.db.is_replay_parsed(replay_id):\n        logger.info(f\"Skipping {replay_id}: already parsed\")\n        return FormatResult(\n            success=True,\n            replay_id=replay_id,\n            dataframe=None,\n            metadata=None,\n            num_rows=0,\n            num_columns=0,\n            num_players=0,\n            error=\"Already parsed (skipped)\"\n        )\n\n    # Parse\n    parse_result = self.parser.parse_file(str(replay_path))\n\n    if not parse_result.success:\n        if self.db:\n            self.db.mark_parse_failed(replay_id, replay_id, parse_result.error)\n        return FormatResult(\n            success=False,\n            replay_id=replay_id,\n            dataframe=None,\n            metadata=None,\n            num_rows=0,\n            num_columns=0,\n            num_players=0,\n            error=parse_result.error\n        )\n\n    # Format\n    format_result = self.formatter.format(parse_result)\n\n    if not format_result.success:\n        if self.db:\n            self.db.mark_parse_failed(replay_id, replay_id, format_result.error)\n        return format_result\n\n    # Save to parquet\n    format_result = self.formatter.save_to_parquet(format_result, output_dir, compression)\n\n    if not format_result.success:\n        if self.db:\n            self.db.mark_parse_failed(replay_id, replay_id, format_result.error)\n        return format_result\n\n    # Register in database (use raw_replay_id as primary key for consistent tracking)\n    if self.db:\n        metadata_json = json.dumps(format_result.metadata) if format_result.metadata else None\n        self.db.add_parsed_replay(\n            replay_id=replay_id,\n            raw_replay_id=replay_id,\n            output_path=format_result.parquet_path,\n            output_format='parquet',\n            fps=self.parser.fps,\n            frame_count=format_result.num_rows,\n            feature_count=format_result.num_columns,\n            file_size_bytes=format_result.parquet_size_bytes,\n            metadata=metadata_json\n        )\n\n    return format_result\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.parsing_pipeline.ParsingPipeline.parse_replays","title":"parse_replays","text":"<pre><code>parse_replays(\n    replay_paths: List[str],\n    output_dir: str,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION,\n) -&gt; PipelineResult\n</code></pre> <p>Parse multiple replay files.</p> <p>Parameters:</p> Name Type Description Default <code>replay_paths</code> <code>List[str]</code> <p>List of paths to .replay files</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output files</p> required <code>compression</code> <code>str</code> <p>Parquet compression algorithm</p> <code>PARQUET_COMPRESSION</code> <p>Returns:</p> Type Description <code>PipelineResult</code> <p>PipelineResult with statistics</p> Source code in <code>impulse/parsing/parsing_pipeline.py</code> <pre><code>def parse_replays(\n    self,\n    replay_paths: List[str],\n    output_dir: str,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION\n) -&gt; PipelineResult:\n    \"\"\"\n    Parse multiple replay files.\n\n    Args:\n        replay_paths: List of paths to .replay files\n        output_dir: Directory to save output files\n        compression: Parquet compression algorithm\n\n    Returns:\n        PipelineResult with statistics\n    \"\"\"\n    total = len(replay_paths)\n    successful = 0\n    skipped = 0\n    failed = 0\n    total_frames = 0\n    total_bytes = 0\n    output_paths = []\n    failed_replays = []\n\n    print()\n    print(\"=\" * 60)\n    print(\"PARSING REPLAYS\")\n    print(\"=\" * 60)\n    print()\n\n    for i, replay_path in enumerate(replay_paths, 1):\n        replay_path = Path(replay_path)\n        replay_id = replay_path.stem\n\n        # Check if already parsed\n        if self.db and self.db.is_replay_parsed(replay_id):\n            self.progress_callback(PipelineProgress(\n                current=i,\n                total=total,\n                replay_id=replay_id,\n                status='skipped',\n                message='Already parsed (database check)'\n            ))\n            skipped += 1\n            print()\n            continue\n\n        try:\n            # Parse\n            self.progress_callback(PipelineProgress(\n                current=i,\n                total=total,\n                replay_id=replay_id,\n                status='parsing',\n                message='Parsing replay...'\n            ))\n\n            parse_result = self.parser.parse_file(str(replay_path))\n\n            if not parse_result.success:\n                raise Exception(parse_result.error)\n\n            # Format\n            self.progress_callback(PipelineProgress(\n                current=i,\n                total=total,\n                replay_id=replay_id,\n                status='formatting',\n                message='Formatting data...'\n            ))\n\n            format_result = self.formatter.format(parse_result)\n\n            if not format_result.success:\n                raise Exception(format_result.error)\n\n            # Save\n            self.progress_callback(PipelineProgress(\n                current=i,\n                total=total,\n                replay_id=replay_id,\n                status='saving',\n                message='Saving to parquet...'\n            ))\n\n            format_result = self.formatter.save_to_parquet(\n                format_result, output_dir, compression\n            )\n\n            if not format_result.success:\n                raise Exception(format_result.error)\n\n            # Register in database (use raw_replay_id as primary key for consistent tracking)\n            if self.db:\n                metadata_json = json.dumps(format_result.metadata) if format_result.metadata else None\n                self.db.add_parsed_replay(\n                    replay_id=replay_id,\n                    raw_replay_id=replay_id,\n                    output_path=format_result.parquet_path,\n                    output_format='parquet',\n                    fps=self.parser.fps,\n                    frame_count=format_result.num_rows,\n                    feature_count=format_result.num_columns,\n                    file_size_bytes=format_result.parquet_size_bytes,\n                    metadata=metadata_json\n                )\n\n            successful += 1\n            total_frames += format_result.num_rows\n            total_bytes += format_result.parquet_size_bytes\n            output_paths.append(format_result.parquet_path)\n\n            self.progress_callback(PipelineProgress(\n                current=i,\n                total=total,\n                replay_id=replay_id,\n                status='complete',\n                message=f'Complete ({format_result.num_rows} frames)',\n                output_path=format_result.parquet_path\n            ))\n            print()\n\n        except Exception as e:\n            error_msg = str(e)\n            logger.error(f\"Failed to parse {replay_id}: {error_msg}\")\n\n            self.progress_callback(PipelineProgress(\n                current=i,\n                total=total,\n                replay_id=replay_id,\n                status='failed',\n                message='Failed',\n                error=error_msg\n            ))\n\n            if self.db:\n                self.db.mark_parse_failed(replay_id, replay_id, error_msg)\n\n            failed += 1\n            failed_replays.append({'replay_id': replay_id, 'error': error_msg})\n            print()\n\n    # Summary\n    print()\n    print(\"=\" * 60)\n    print(\"PARSING SUMMARY\")\n    print(\"=\" * 60)\n    print(f\"Total replays: {total}\")\n    print(f\"Successfully parsed: {successful}\")\n    print(f\"Skipped (already parsed): {skipped}\")\n    print(f\"Failed: {failed}\")\n    print(f\"Total frames: {total_frames:,}\")\n    print(f\"Total output size: {total_bytes / (1024**2):.2f} MB\")\n\n    if self.db:\n        print()\n        print(\"DATABASE STATISTICS\")\n        print(\"-\" * 60)\n        stats = self.db.get_parse_stats()\n        for key, value in stats.items():\n            print(f\"{key}: {value}\")\n\n    return PipelineResult(\n        total_replays=total,\n        successful=successful,\n        skipped=skipped,\n        failed=failed,\n        total_frames=total_frames,\n        total_bytes=total_bytes,\n        output_paths=output_paths,\n        failed_replays=failed_replays\n    )\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.parsing_pipeline.ParsingPipeline.parse_unparsed","title":"parse_unparsed","text":"<pre><code>parse_unparsed(\n    raw_replays_dir: str,\n    output_dir: str,\n    limit: Optional[int] = None,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION,\n) -&gt; PipelineResult\n</code></pre> <p>Parse all downloaded replays that haven't been parsed yet.</p> <p>Queries the database for unparsed replays and processes them.</p> <p>Parameters:</p> Name Type Description Default <code>raw_replays_dir</code> <code>str</code> <p>Directory containing raw .replay files</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output files</p> required <code>limit</code> <code>Optional[int]</code> <p>Maximum number of replays to parse (None for all)</p> <code>None</code> <code>compression</code> <code>str</code> <p>Parquet compression algorithm</p> <code>PARQUET_COMPRESSION</code> <p>Returns:</p> Type Description <code>PipelineResult</code> <p>PipelineResult with statistics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If database is not configured</p> Source code in <code>impulse/parsing/parsing_pipeline.py</code> <pre><code>def parse_unparsed(\n    self,\n    raw_replays_dir: str,\n    output_dir: str,\n    limit: Optional[int] = None,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION\n) -&gt; PipelineResult:\n    \"\"\"\n    Parse all downloaded replays that haven't been parsed yet.\n\n    Queries the database for unparsed replays and processes them.\n\n    Args:\n        raw_replays_dir: Directory containing raw .replay files\n        output_dir: Directory to save output files\n        limit: Maximum number of replays to parse (None for all)\n        compression: Parquet compression algorithm\n\n    Returns:\n        PipelineResult with statistics\n\n    Raises:\n        ValueError: If database is not configured\n    \"\"\"\n    if not self.db:\n        raise ValueError(\"Database required for parse_unparsed(). \"\n                       \"Initialize ParsingPipeline with a database.\")\n\n    # Get unparsed replays from database\n    unparsed = self.db.get_unparsed_replays(limit=limit)\n\n    if not unparsed:\n        print(\"No unparsed replays found.\")\n        return PipelineResult(\n            total_replays=0,\n            successful=0,\n            skipped=0,\n            failed=0,\n            total_frames=0,\n            total_bytes=0,\n            output_paths=[],\n            failed_replays=[]\n        )\n\n    print(f\"Found {len(unparsed)} unparsed replays\")\n\n    # Build file paths\n    raw_dir = Path(raw_replays_dir)\n    replay_paths = []\n\n    for replay_info in unparsed:\n        replay_id = replay_info['replay_id']\n        # Try to find the replay file\n        # First check storage_key if available\n        storage_key = replay_info.get('storage_key')\n        if storage_key:\n            # storage_key might be a full path or relative path\n            replay_path = raw_dir / storage_key\n            if not replay_path.exists():\n                # Try just the filename\n                replay_path = raw_dir / f\"{replay_id}.replay\"\n        else:\n            replay_path = raw_dir / f\"{replay_id}.replay\"\n\n        # Search recursively if not found\n        if not replay_path.exists():\n            matches = list(raw_dir.rglob(f\"{replay_id}.replay\"))\n            if matches:\n                replay_path = matches[0]\n\n        if replay_path.exists():\n            replay_paths.append(str(replay_path))\n        else:\n            logger.warning(f\"Replay file not found for {replay_id}\")\n\n    if not replay_paths:\n        print(\"No replay files found on disk.\")\n        return PipelineResult(\n            total_replays=0,\n            successful=0,\n            skipped=0,\n            failed=0,\n            total_frames=0,\n            total_bytes=0,\n            output_paths=[],\n            failed_replays=[]\n        )\n\n    print(f\"Found {len(replay_paths)} replay files on disk\")\n\n    return self.parse_replays(replay_paths, output_dir, compression)\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.parsing_pipeline.ParsingPipeline.retry_failed_parses","title":"retry_failed_parses","text":"<pre><code>retry_failed_parses(\n    raw_replays_dir: str,\n    output_dir: str,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION,\n) -&gt; PipelineResult\n</code></pre> <p>Retry parsing replays that previously failed.</p> <p>Parameters:</p> Name Type Description Default <code>raw_replays_dir</code> <code>str</code> <p>Directory containing raw .replay files</p> required <code>output_dir</code> <code>str</code> <p>Directory to save output files</p> required <code>compression</code> <code>str</code> <p>Parquet compression algorithm</p> <code>PARQUET_COMPRESSION</code> <p>Returns:</p> Type Description <code>PipelineResult</code> <p>PipelineResult with statistics</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If database is not configured</p> Source code in <code>impulse/parsing/parsing_pipeline.py</code> <pre><code>def retry_failed_parses(\n    self,\n    raw_replays_dir: str,\n    output_dir: str,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION\n) -&gt; PipelineResult:\n    \"\"\"\n    Retry parsing replays that previously failed.\n\n    Args:\n        raw_replays_dir: Directory containing raw .replay files\n        output_dir: Directory to save output files\n        compression: Parquet compression algorithm\n\n    Returns:\n        PipelineResult with statistics\n\n    Raises:\n        ValueError: If database is not configured\n    \"\"\"\n    if not self.db:\n        raise ValueError(\"Database required for retry_failed_parses(). \"\n                       \"Initialize ParsingPipeline with a database.\")\n\n    # Get failed parses\n    failed = self.db.get_failed_parses()\n\n    if not failed:\n        print(\"No failed parses to retry.\")\n        return PipelineResult(\n            total_replays=0,\n            successful=0,\n            skipped=0,\n            failed=0,\n            total_frames=0,\n            total_bytes=0,\n            output_paths=[],\n            failed_replays=[]\n        )\n\n    print(f\"Found {len(failed)} failed parses to retry\")\n\n    # Build file paths\n    raw_dir = Path(raw_replays_dir)\n    replay_paths = []\n\n    for parse_info in failed:\n        replay_id = parse_info['raw_replay_id']\n        replay_path = raw_dir / f\"{replay_id}.replay\"\n\n        # Search recursively if not found\n        if not replay_path.exists():\n            matches = list(raw_dir.rglob(f\"{replay_id}.replay\"))\n            if matches:\n                replay_path = matches[0]\n\n        if replay_path.exists():\n            replay_paths.append(str(replay_path))\n        else:\n            logger.warning(f\"Replay file not found for {replay_id}\")\n\n    if not replay_paths:\n        print(\"No replay files found for failed parses.\")\n        return PipelineResult(\n            total_replays=0,\n            successful=0,\n            skipped=0,\n            failed=0,\n            total_frames=0,\n            total_bytes=0,\n            output_paths=[],\n            failed_replays=[]\n        )\n\n    return self.parse_replays(replay_paths, output_dir, compression)\n</code></pre>"},{"location":"parsing/api-reference/#parseresultformatter","title":"ParseResultFormatter","text":""},{"location":"parsing/api-reference/#impulse.parsing.parse_result_formatter.ParseResultFormatter","title":"ParseResultFormatter","text":"<pre><code>ParseResultFormatter(\n    max_players: int = PipelineConfig.SCHEMA_MAX_PLAYERS,\n)\n</code></pre> <p>Formats ParseResult objects into structured data (DataFrame/Parquet).</p> <p>Handles: - Pipeline quality validation (frame counts, player counts, NaN/Inf detection) - Feature deduplication (removes redundant position/rotation columns) - Schema standardization (pads to max player count for fixed schema) - DataFrame creation with proper column names - Parquet export with metadata - Player mapping extraction</p> <p>Initialize the formatter.</p> <p>Parameters:</p> Name Type Description Default <code>max_players</code> <code>int</code> <p>Number of player columns to allocate in Parquet schema</p> <code>SCHEMA_MAX_PLAYERS</code> Source code in <code>impulse/parsing/parse_result_formatter.py</code> <pre><code>def __init__(self, max_players: int = PipelineConfig.SCHEMA_MAX_PLAYERS):\n    \"\"\"\n    Initialize the formatter.\n\n    Args:\n        max_players: Number of player columns to allocate in Parquet schema\n    \"\"\"\n    self.max_players = max_players\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.parse_result_formatter.ParseResultFormatter.validate_quality","title":"validate_quality","text":"<pre><code>validate_quality(\n    parse_result: ParseResult,\n    config: PipelineConfig = PipelineConfig(),\n) -&gt; Tuple[bool, List[str], Dict[str, Any]]\n</code></pre> <p>Validate parsed data meets pipeline quality standards.</p> <p>Parameters:</p> Name Type Description Default <code>parse_result</code> <code>ParseResult</code> <p>Result from ReplayParser</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration with validation thresholds</p> <code>PipelineConfig()</code> <p>Returns:</p> Type Description <code>bool</code> <p>Tuple of (is_valid, warnings, validation_info)</p> <code>List[str]</code> <ul> <li>is_valid: False only for hard failures (frame/player count out of bounds)</li> </ul> <code>Dict[str, Any]</code> <ul> <li>warnings: List of warning messages (NaN/Inf detection, etc.)</li> </ul> <code>Tuple[bool, List[str], Dict[str, Any]]</code> <ul> <li>validation_info: Dict with validation statistics</li> </ul> Source code in <code>impulse/parsing/parse_result_formatter.py</code> <pre><code>def validate_quality(self,\n                    parse_result: ParseResult,\n                    config: PipelineConfig = PipelineConfig()) -&gt; Tuple[bool, List[str], Dict[str, Any]]:\n    \"\"\"\n    Validate parsed data meets pipeline quality standards.\n\n    Args:\n        parse_result: Result from ReplayParser\n        config: Pipeline configuration with validation thresholds\n\n    Returns:\n        Tuple of (is_valid, warnings, validation_info)\n        - is_valid: False only for hard failures (frame/player count out of bounds)\n        - warnings: List of warning messages (NaN/Inf detection, etc.)\n        - validation_info: Dict with validation statistics\n    \"\"\"\n    warnings = []\n    validation_info = {\n        'has_nan': False,\n        'has_inf': False,\n        'nan_count': 0,\n        'inf_count': 0\n    }\n\n    # Frame count validation (hard failure)\n    if parse_result.num_frames &lt; config.MIN_FRAMES:\n        return False, [f\"Too few frames: {parse_result.num_frames} &lt; {config.MIN_FRAMES}\"], validation_info\n\n    if parse_result.num_frames &gt; config.MAX_FRAMES:\n        return False, [f\"Too many frames: {parse_result.num_frames} &gt; {config.MAX_FRAMES}\"], validation_info\n\n    # Player count validation (hard failure)\n    if parse_result.num_players &lt; config.MIN_PLAYERS:\n        return False, [f\"Too few players: {parse_result.num_players} &lt; {config.MIN_PLAYERS}\"], validation_info\n\n    if parse_result.num_players &gt; config.MAX_PLAYERS:\n        return False, [f\"Too many players: {parse_result.num_players} &gt; {config.MAX_PLAYERS}\"], validation_info\n\n    # NaN detection (warning only)\n    if np.isnan(parse_result.array).any():\n        nan_count = int(np.isnan(parse_result.array).sum())\n        validation_info['has_nan'] = True\n        validation_info['nan_count'] = nan_count\n        warnings.append(f\"Array contains {nan_count} NaN values\")\n\n    # Inf detection (warning only)\n    if np.isinf(parse_result.array).any():\n        inf_count = int(np.isinf(parse_result.array).sum())\n        validation_info['has_inf'] = True\n        validation_info['inf_count'] = inf_count\n        warnings.append(f\"Array contains {inf_count} Inf values\")\n\n    # Column count validation (warning only)\n    if parse_result.global_features and parse_result.player_features:\n        expected_cols = self._get_expected_column_count(\n            parse_result.global_features,\n            parse_result.player_features,\n            parse_result.num_players\n        )\n        if parse_result.num_features != expected_cols:\n            warnings.append(\n                f\"Column count mismatch: expected {expected_cols}, got {parse_result.num_features}\"\n            )\n\n    return True, warnings, validation_info\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.parse_result_formatter.ParseResultFormatter.format","title":"format","text":"<pre><code>format(\n    parse_result: ParseResult,\n    config: PipelineConfig = PipelineConfig(),\n) -&gt; FormatResult\n</code></pre> <p>Format a parse result into a structured DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>parse_result</code> <code>ParseResult</code> <p>Result from ReplayParser</p> required <code>config</code> <code>PipelineConfig</code> <p>Pipeline configuration for validation and deduplication</p> <code>PipelineConfig()</code> <p>Returns:</p> Type Description <code>FormatResult</code> <p>FormatResult with formatted data or error information</p> Source code in <code>impulse/parsing/parse_result_formatter.py</code> <pre><code>def format(self, parse_result: ParseResult, config: PipelineConfig = PipelineConfig()) -&gt; FormatResult:\n    \"\"\"\n    Format a parse result into a structured DataFrame.\n\n    Args:\n        parse_result: Result from ReplayParser\n        config: Pipeline configuration for validation and deduplication\n\n    Returns:\n        FormatResult with formatted data or error information\n    \"\"\"\n    if not parse_result.success:\n        return FormatResult(\n            success=False,\n            replay_id=Path(parse_result.replay_path).stem,\n            dataframe=None,\n            metadata=None,\n            num_rows=-1,\n            num_columns=-1,\n            num_players=-1,\n            error=f\"Cannot format failed parse: {parse_result.error}\"\n        )\n\n    # Validate quality\n    is_valid, warnings, validation_info = self.validate_quality(parse_result, config)\n    if not is_valid:\n        # Hard validation failure (frame count or player count out of bounds)\n        return FormatResult(\n            success=False,\n            replay_id=Path(parse_result.replay_path).stem,\n            dataframe=None,\n            metadata=None,\n            num_rows=parse_result.num_frames,\n            num_columns=parse_result.num_features,\n            num_players=parse_result.num_players,\n            validation_warnings=warnings,\n            has_nan=validation_info['has_nan'],\n            has_inf=validation_info['has_inf'],\n            nan_count=validation_info['nan_count'],\n            inf_count=validation_info['inf_count'],\n            error=f\"Validation failed: {warnings[0] if warnings else 'Unknown error'}\"\n        )\n\n    try:\n        # Use filename stem as replay_id for consistent naming\n        replay_id = Path(parse_result.replay_path).stem\n\n        # Deduplicate features (pass feature lists directly)\n        deduplicated_array, column_names = self._deduplicate_features(\n            parse_result.array,\n            parse_result.global_features,\n            parse_result.player_features,\n            parse_result.num_players,\n            config\n        )\n\n        # Pad to max players\n        padded_array, padded_columns = self._pad_to_max_players(\n            deduplicated_array,\n            column_names,\n            parse_result.num_players\n        )\n\n        # Create DataFrame\n        df = pd.DataFrame(padded_array, columns=padded_columns)\n\n        # Add frame index column\n        df.insert(0, 'frame', range(len(df)))\n\n        # Extract metadata\n        clean_metadata = self._extract_metadata(parse_result)\n\n        return FormatResult(\n            success=True,\n            replay_id=replay_id,\n            dataframe=df,\n            metadata=clean_metadata,\n            num_rows=len(df),\n            num_columns=len(df.columns),\n            num_players=parse_result.num_players,\n            validation_warnings=warnings,  # Include warnings even on success\n            has_nan=validation_info['has_nan'],\n            has_inf=validation_info['has_inf'],\n            nan_count=validation_info['nan_count'],\n            inf_count=validation_info['inf_count']\n        )\n\n    except Exception as e:\n        return FormatResult(\n            success=False,\n            replay_id=Path(parse_result.replay_path).stem,\n            dataframe=None,\n            metadata=None,\n            num_rows=0,\n            num_columns=0,\n            num_players=0,\n            error=f\"Formatting failed: {str(e)}\"\n        )\n</code></pre>"},{"location":"parsing/api-reference/#impulse.parsing.parse_result_formatter.ParseResultFormatter.save_to_parquet","title":"save_to_parquet","text":"<pre><code>save_to_parquet(\n    format_result: FormatResult,\n    output_dir: str,\n    compression: str = PipelineConfig.PARQUET_COMPRESSION,\n) -&gt; FormatResult\n</code></pre> <p>Save formatted data to Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>format_result</code> <code>FormatResult</code> <p>Result from format()</p> required <code>output_dir</code> <code>str</code> <p>Directory to save files</p> required <code>compression</code> <code>str</code> <p>Compression algorithm ('snappy', 'gzip', 'zstd')</p> <code>PARQUET_COMPRESSION</code> <p>Returns:</p> Type Description <code>FormatResult</code> <p>Updated FormatResult with file paths and sizes</p> Source code in <code>impulse/parsing/parse_result_formatter.py</code> <pre><code>def save_to_parquet(self,\n                   format_result: FormatResult,\n                   output_dir: str,\n                   compression: str = PipelineConfig.PARQUET_COMPRESSION) -&gt; FormatResult:\n    \"\"\"\n    Save formatted data to Parquet file.\n\n    Args:\n        format_result: Result from format()\n        output_dir: Directory to save files\n        compression: Compression algorithm ('snappy', 'gzip', 'zstd')\n\n    Returns:\n        Updated FormatResult with file paths and sizes\n    \"\"\"\n    if not format_result.success:\n        return format_result\n\n    try:\n        output_path = Path(output_dir)\n        output_path.mkdir(parents=True, exist_ok=True)\n\n        # Save Parquet\n        parquet_file = output_path / f\"{format_result.replay_id}.parquet\"\n        format_result.dataframe.to_parquet(\n            parquet_file,\n            compression=compression,\n            index=False\n        )\n        format_result.parquet_path = str(parquet_file)\n        format_result.parquet_size_bytes = parquet_file.stat().st_size\n\n        # Save metadata\n        metadata_file = output_path / f\"{format_result.replay_id}.metadata.json\"\n        with open(metadata_file, 'w') as f:\n            json.dump(format_result.metadata, f, indent=2)\n        format_result.metadata_path = str(metadata_file)\n        format_result.metadata_size_bytes = metadata_file.stat().st_size\n\n        return format_result\n\n    except Exception as e:\n        format_result.success = False\n        format_result.error = f\"Save failed: {str(e)}\"\n        return format_result\n</code></pre>"},{"location":"parsing/features/","title":"Feature Extraction","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"},{"location":"parsing/output-formats/","title":"Output Formats","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"},{"location":"parsing/overview/","title":"Parsing Overview","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"},{"location":"reference/changelog/","title":"Changelog","text":""},{"location":"reference/changelog/#010","title":"0.1.0","text":"<p>Initial release.</p>"},{"location":"reference/database/","title":"Database Reference","text":"<p>Work in progress</p> <p>This page is being written. Check back soon.</p>"}]}